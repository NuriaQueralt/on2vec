{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems with on2vec\n",
    "\n",
    "This notebook demonstrates how to build intelligent recommendation systems using on2vec embeddings. We'll show how to:\n",
    "\n",
    "1. Recommend related concepts based on user interests\n",
    "2. Suggest complementary tools and methods for research workflows\n",
    "3. Build content-based filtering systems for scientific resources\n",
    "4. Create collaborative filtering using concept similarity\n",
    "5. Implement hybrid recommendation approaches\n",
    "6. Evaluate recommendation quality and diversity\n",
    "\n",
    "## Use Case: Scientific Resource Discovery\n",
    "Researchers often struggle to discover relevant tools, datasets, and methods for their work. Traditional keyword-based search misses semantically related resources. Embedding-based recommendations can suggest relevant resources based on conceptual similarity, helping researchers discover tools and data they might not have found otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import umap\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# on2vec imports\n",
    "from on2vec import (\n",
    "    load_embeddings_as_dataframe,\n",
    "    train_ontology_embeddings,\n",
    "    embed_ontology_with_model\n",
    ")\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Recommendation Data\n",
    "\n",
    "Load ontology embeddings and create synthetic user interaction data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_recommendation_data(ontology_file):\n",
    "    \"\"\"Prepare embeddings and synthetic user data for recommendations.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(ontology_file):\n",
    "        print(f\"‚ùå Ontology file not found: {ontology_file}\")\n",
    "        return None\n",
    "    \n",
    "    base_name = Path(ontology_file).stem\n",
    "    model_file = f\"{base_name}_recommendation_model.pt\"\n",
    "    embedding_file = f\"{base_name}_recommendation_embeddings.parquet\"\n",
    "    \n",
    "    print(f\"üîÑ Preparing recommendation data for {ontology_file}...\")\n",
    "    \n",
    "    # Train model optimized for recommendation tasks\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"  Training recommendation model...\")\n",
    "        result = train_ontology_embeddings(\n",
    "            owl_file=ontology_file,\n",
    "            model_output=model_file,\n",
    "            model_type=\"gat\",  # GAT for attention-based recommendations\n",
    "            hidden_dim=256,\n",
    "            out_dim=128,\n",
    "            epochs=100,\n",
    "            loss_fn=\"cosine\",  # Cosine loss for similarity tasks\n",
    "            learning_rate=0.01\n",
    "            text_model_type=\"sentence_transformer\",\n",
    "            text_model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    if not os.path.exists(embedding_file):\n",
    "        print(f\"  Generating embeddings...\")\n",
    "        embed_result = embed_ontology_with_model(\n",
    "            model_path=model_file,\n",
    "            owl_file=ontology_file,\n",
    "            output_file=embedding_file\n",
    "        )\n",
    "    \n",
    "    # Load embeddings\n",
    "    df, metadata = load_embeddings_as_dataframe(embedding_file, return_metadata=True)\n",
    "    embeddings = np.stack(df['embedding'].to_numpy())\n",
    "    node_ids = df['node_id'].to_numpy()\n",
    "    \n",
    "    # Extract concept names and categories\n",
    "    concept_names = []\n",
    "    concept_categories = []\n",
    "    \n",
    "    for node_id in node_ids:\n",
    "        # Extract name\n",
    "        if '#' in node_id:\n",
    "            name = node_id.split('#')[-1]\n",
    "        else:\n",
    "            name = node_id.split('/')[-1]\n",
    "        \n",
    "        clean_name = name.replace('_', ' ').replace('-', ' ')\n",
    "        concept_names.append(clean_name)\n",
    "        \n",
    "        # Categorize based on name (simple heuristic)\n",
    "        name_lower = clean_name.lower()\n",
    "        if any(word in name_lower for word in ['data', 'format', 'file']):\n",
    "            category = 'data_formats'\n",
    "        elif any(word in name_lower for word in ['analysis', 'method', 'algorithm']):\n",
    "            category = 'methods'\n",
    "        elif any(word in name_lower for word in ['protein', 'gene', 'sequence']):\n",
    "            category = 'biology'\n",
    "        elif any(word in name_lower for word in ['tool', 'software', 'program']):\n",
    "            category = 'tools'\n",
    "        elif any(word in name_lower for word in ['database', 'repository']):\n",
    "            category = 'databases'\n",
    "        else:\n",
    "            category = 'general'\n",
    "        \n",
    "        concept_categories.append(category)\n",
    "    \n",
    "    print(f\"  ‚úì Loaded {len(node_ids)} concepts with {embeddings.shape[1]}D embeddings\")\n",
    "    \n",
    "    # Create concept DataFrame\n",
    "    concepts_df = pd.DataFrame({\n",
    "        'concept_id': range(len(node_ids)),\n",
    "        'node_id': node_ids,\n",
    "        'name': concept_names,\n",
    "        'category': concept_categories\n",
    "    })\n",
    "    \n",
    "    category_counts = Counter(concept_categories)\n",
    "    print(f\"  ‚úì Categories: {dict(category_counts)}\")\n",
    "    \n",
    "    return {\n",
    "        'ontology_file': ontology_file,\n",
    "        'embeddings': embeddings,\n",
    "        'concepts_df': concepts_df,\n",
    "        'node_ids': node_ids,\n",
    "        'concept_names': concept_names,\n",
    "        'concept_categories': concept_categories,\n",
    "        'metadata': metadata,\n",
    "        'similarity_matrix': cosine_similarity(embeddings),\n",
    "        'model_file': model_file,\n",
    "        'embedding_file': embedding_file\n",
    "    }\n",
    "\n",
    "def generate_synthetic_user_interactions(rec_data, n_users=50, interactions_per_user_range=(5, 25)):\n",
    "    \"\"\"Generate synthetic user interaction data for demonstration.\"\"\"\n",
    "    \n",
    "    print(f\"üé≠ Generating synthetic user interactions...\")\n",
    "    \n",
    "    n_concepts = len(rec_data['concepts_df'])\n",
    "    categories = rec_data['concepts_df']['category'].unique()\n",
    "    \n",
    "    users_data = []\n",
    "    interactions_data = []\n",
    "    \n",
    "    # Create user profiles with category preferences\n",
    "    for user_id in range(n_users):\n",
    "        # Assign user type and preferences\n",
    "        user_types = ['biologist', 'data_scientist', 'bioinformatician', 'computational_biologist']\n",
    "        user_type = np.random.choice(user_types)\n",
    "        \n",
    "        # Define category preferences by user type\n",
    "        if user_type == 'biologist':\n",
    "            preferred_categories = ['biology', 'databases', 'general']\n",
    "            category_weights = [0.5, 0.3, 0.2]\n",
    "        elif user_type == 'data_scientist':\n",
    "            preferred_categories = ['methods', 'data_formats', 'tools']\n",
    "            category_weights = [0.4, 0.4, 0.2]\n",
    "        elif user_type == 'bioinformatician':\n",
    "            preferred_categories = ['tools', 'data_formats', 'methods', 'biology']\n",
    "            category_weights = [0.3, 0.3, 0.2, 0.2]\n",
    "        else:  # computational_biologist\n",
    "            preferred_categories = ['methods', 'biology', 'tools']\n",
    "            category_weights = [0.4, 0.3, 0.3]\n",
    "        \n",
    "        users_data.append({\n",
    "            'user_id': user_id,\n",
    "            'user_type': user_type,\n",
    "            'preferred_categories': preferred_categories,\n",
    "            'category_weights': category_weights\n",
    "        })\n",
    "        \n",
    "        # Generate interactions based on preferences\n",
    "        n_interactions = np.random.randint(*interactions_per_user_range)\n",
    "        \n",
    "        for _ in range(n_interactions):\n",
    "            # Choose category based on user preferences\n",
    "            chosen_category = np.random.choice(preferred_categories, p=category_weights)\n",
    "            \n",
    "            # Choose concept from that category\n",
    "            category_concepts = rec_data['concepts_df'][rec_data['concepts_df']['category'] == chosen_category]\n",
    "            \n",
    "            if not category_concepts.empty:\n",
    "                concept_row = category_concepts.sample(1).iloc[0]\n",
    "                \n",
    "                # Generate interaction strength (rating)\n",
    "                base_rating = np.random.beta(2, 1) * 5  # Skewed towards higher ratings\n",
    "                rating = np.clip(base_rating, 1, 5)\n",
    "                \n",
    "                # Add some noise based on actual concept similarity to user interests\n",
    "                concept_embedding = rec_data['embeddings'][concept_row['concept_id']]\n",
    "                \n",
    "                # Calculate 'fit' with user preferences (simplified)\n",
    "                category_fit = category_weights[preferred_categories.index(chosen_category)]\n",
    "                final_rating = rating * category_fit + np.random.normal(0, 0.3)\n",
    "                final_rating = np.clip(final_rating, 1, 5)\n",
    "                \n",
    "                interactions_data.append({\n",
    "                    'user_id': user_id,\n",
    "                    'concept_id': concept_row['concept_id'],\n",
    "                    'concept_name': concept_row['name'],\n",
    "                    'category': chosen_category,\n",
    "                    'rating': final_rating,\n",
    "                    'interaction_type': np.random.choice(['view', 'bookmark', 'use', 'share'], \n",
    "                                                        p=[0.4, 0.3, 0.2, 0.1])\n",
    "                })\n",
    "    \n",
    "    users_df = pd.DataFrame(users_data)\n",
    "    interactions_df = pd.DataFrame(interactions_data)\n",
    "    \n",
    "    print(f\"  ‚úì Generated {len(users_df)} users with {len(interactions_df)} interactions\")\n",
    "    print(f\"  ‚úì Average interactions per user: {len(interactions_df) / len(users_df):.1f}\")\n",
    "    \n",
    "    return users_df, interactions_df\n",
    "\n",
    "# Prepare recommendation data\n",
    "ontology_files = ['EDAM.owl', 'cvdo.owl']\n",
    "rec_data = None\n",
    "\n",
    "for ont_file in ontology_files:\n",
    "    if os.path.exists(ont_file):\n",
    "        rec_data = prepare_recommendation_data(ont_file)\n",
    "        if rec_data:\n",
    "            print(f\"\\n‚úÖ Recommendation data ready:\")\n",
    "            print(f\"  ‚Ä¢ Ontology: {ont_file}\")\n",
    "            print(f\"  ‚Ä¢ Concepts: {len(rec_data['concept_names']):,}\")\n",
    "            print(f\"  ‚Ä¢ Embeddings: {rec_data['embeddings'].shape[1]}D\")\n",
    "            break\n",
    "\n",
    "if rec_data:\n",
    "    # Generate synthetic user interactions\n",
    "    users_df, interactions_df = generate_synthetic_user_interactions(rec_data)\n",
    "    \n",
    "    print(f\"\\nüìä User Interaction Summary:\")\n",
    "    print(f\"  ‚Ä¢ User types: {users_df['user_type'].value_counts().to_dict()}\")\n",
    "    print(f\"  ‚Ä¢ Interaction types: {interactions_df['interaction_type'].value_counts().to_dict()}\")\n",
    "    print(f\"  ‚Ä¢ Average rating: {interactions_df['rating'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"‚ùå No suitable ontology files found for recommendation demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Content-Based Recommendation System\n",
    "\n",
    "Build a content-based recommender using concept embeddings to find similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecommender:\n",
    "    def __init__(self, rec_data):\n",
    "        \"\"\"Initialize content-based recommender with concept embeddings.\"\"\"\n",
    "        self.rec_data = rec_data\n",
    "        self.embeddings = rec_data['embeddings']\n",
    "        self.concepts_df = rec_data['concepts_df']\n",
    "        self.similarity_matrix = rec_data['similarity_matrix']\n",
    "        \n",
    "        print(f\"üéØ Content-based recommender initialized with {len(self.concepts_df)} concepts\")\n",
    "    \n",
    "    def recommend_similar_concepts(self, concept_ids, top_k=10, diversity_weight=0.3):\n",
    "        \"\"\"Recommend concepts similar to given input concepts.\"\"\"\n",
    "        if isinstance(concept_ids, int):\n",
    "            concept_ids = [concept_ids]\n",
    "        \n",
    "        # Calculate average similarity for multi-concept input\n",
    "        similarities = np.zeros(len(self.concepts_df))\n",
    "        \n",
    "        for concept_id in concept_ids:\n",
    "            if 0 <= concept_id < len(self.similarity_matrix):\n",
    "                similarities += self.similarity_matrix[concept_id]\n",
    "        \n",
    "        similarities /= len(concept_ids)\n",
    "        \n",
    "        # Remove input concepts from recommendations\n",
    "        for concept_id in concept_ids:\n",
    "            similarities[concept_id] = -1\n",
    "        \n",
    "        # Apply diversity weighting\n",
    "        if diversity_weight > 0:\n",
    "            similarities = self._apply_diversity_weighting(\n",
    "                similarities, concept_ids, diversity_weight\n",
    "            )\n",
    "        \n",
    "        # Get top recommendations\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] > 0:  # Valid recommendation\n",
    "                concept_row = self.concepts_df.iloc[idx]\n",
    "                recommendations.append({\n",
    "                    'concept_id': concept_row['concept_id'],\n",
    "                    'name': concept_row['name'],\n",
    "                    'category': concept_row['category'],\n",
    "                    'similarity': float(similarities[idx]),\n",
    "                    'recommendation_type': 'content_based'\n",
    "                })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _apply_diversity_weighting(self, similarities, seed_concept_ids, diversity_weight):\n",
    "        \"\"\"Apply diversity weighting to promote variety in recommendations.\"\"\"\n",
    "        # Get categories of seed concepts\n",
    "        seed_categories = set()\n",
    "        for concept_id in seed_concept_ids:\n",
    "            seed_categories.add(self.concepts_df.iloc[concept_id]['category'])\n",
    "        \n",
    "        # Boost concepts from different categories\n",
    "        diversified_similarities = similarities.copy()\n",
    "        \n",
    "        for idx, similarity in enumerate(similarities):\n",
    "            if similarity > 0:  # Valid concept\n",
    "                concept_category = self.concepts_df.iloc[idx]['category']\n",
    "                \n",
    "                if concept_category not in seed_categories:\n",
    "                    # Boost diversity\n",
    "                    diversified_similarities[idx] = similarity * (1 + diversity_weight)\n",
    "                else:\n",
    "                    # Slight penalty for same category\n",
    "                    diversified_similarities[idx] = similarity * (1 - diversity_weight * 0.2)\n",
    "        \n",
    "        return diversified_similarities\n",
    "    \n",
    "    def recommend_by_category(self, target_category, exclude_concepts=None, top_k=10):\n",
    "        \"\"\"Recommend top concepts from a specific category.\"\"\"\n",
    "        category_concepts = self.concepts_df[self.concepts_df['category'] == target_category]\n",
    "        \n",
    "        if exclude_concepts:\n",
    "            category_concepts = category_concepts[~category_concepts['concept_id'].isin(exclude_concepts)]\n",
    "        \n",
    "        if category_concepts.empty:\n",
    "            return []\n",
    "        \n",
    "        # For category-based recommendations, we can use various strategies:\n",
    "        # 1. Random sampling\n",
    "        # 2. Centrality in embedding space\n",
    "        # 3. Diversity within category\n",
    "        \n",
    "        # Use embedding centrality (concepts closest to category centroid)\n",
    "        category_indices = category_concepts['concept_id'].values\n",
    "        category_embeddings = self.embeddings[category_indices]\n",
    "        \n",
    "        # Calculate centroid\n",
    "        centroid = np.mean(category_embeddings, axis=0)\n",
    "        \n",
    "        # Calculate distances to centroid\n",
    "        distances_to_centroid = cosine_similarity(\n",
    "            category_embeddings, centroid.reshape(1, -1)\n",
    "        ).flatten()\n",
    "        \n",
    "        # Get top concepts (closest to centroid = most representative)\n",
    "        top_indices = np.argsort(distances_to_centroid)[::-1][:top_k]\n",
    "        \n",
    "        recommendations = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            concept_row = category_concepts.iloc[idx]\n",
    "            recommendations.append({\n",
    "                'concept_id': concept_row['concept_id'],\n",
    "                'name': concept_row['name'],\n",
    "                'category': concept_row['category'],\n",
    "                'centrality_score': float(distances_to_centroid[idx]),\n",
    "                'recommendation_type': 'category_based'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize content-based recommender\n",
    "if rec_data:\n",
    "    content_recommender = ContentBasedRecommender(rec_data)\n",
    "    \n",
    "    # Demo: Recommend similar concepts\n",
    "    print(\"\\nüéØ CONTENT-BASED RECOMMENDATION EXAMPLES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Example 1: Single concept recommendation\n",
    "    sample_concept_id = np.random.choice(len(rec_data['concepts_df']))\n",
    "    sample_concept = rec_data['concepts_df'].iloc[sample_concept_id]\n",
    "    \n",
    "    print(f\"\\nüìù Seed Concept: {sample_concept['name']} ({sample_concept['category']})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    similar_recommendations = content_recommender.recommend_similar_concepts(\n",
    "        sample_concept_id, top_k=8, diversity_weight=0.3\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(similar_recommendations[:5]):\n",
    "        print(f\"  {i+1}. {rec['name']:30} [{rec['category']:12}] (sim: {rec['similarity']:.3f})\")\n",
    "    \n",
    "    # Example 2: Multi-concept recommendation\n",
    "    seed_concepts = np.random.choice(len(rec_data['concepts_df']), 3, replace=False)\n",
    "    seed_names = [rec_data['concepts_df'].iloc[i]['name'] for i in seed_concepts]\n",
    "    \n",
    "    print(f\"\\nüìù Multi-concept Seeds: {', '.join(seed_names[:2])}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    multi_recommendations = content_recommender.recommend_similar_concepts(\n",
    "        seed_concepts.tolist(), top_k=8, diversity_weight=0.4\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(multi_recommendations[:5]):\n",
    "        print(f\"  {i+1}. {rec['name']:30} [{rec['category']:12}] (sim: {rec['similarity']:.3f})\")\n",
    "    \n",
    "    # Example 3: Category-based recommendations\n",
    "    target_category = np.random.choice(rec_data['concepts_df']['category'].unique())\n",
    "    \n",
    "    print(f\"\\nüìù Category Recommendations: {target_category}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    category_recommendations = content_recommender.recommend_by_category(\n",
    "        target_category, top_k=5\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(category_recommendations):\n",
    "        print(f\"  {i+1}. {rec['name']:30} (centrality: {rec['centrality_score']:.3f})\")\n",
    "        \n",
    "else:\n",
    "    print(\"Content-based recommender not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Collaborative Filtering System\n",
    "\n",
    "Build a collaborative filtering system using user interaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringRecommender:\n",
    "    def __init__(self, rec_data, interactions_df):\n",
    "        \"\"\"Initialize collaborative filtering recommender.\"\"\"\n",
    "        self.rec_data = rec_data\n",
    "        self.interactions_df = interactions_df\n",
    "        self.concepts_df = rec_data['concepts_df']\n",
    "        \n",
    "        # Build user-item interaction matrix\n",
    "        self.user_item_matrix, self.user_ids, self.item_ids = self._build_interaction_matrix()\n",
    "        \n",
    "        # Calculate user and item similarity matrices\n",
    "        self.user_similarity = self._calculate_user_similarity()\n",
    "        self.item_similarity = self._calculate_item_similarity()\n",
    "        \n",
    "        print(f\"ü§ù Collaborative filtering initialized:\")\n",
    "        print(f\"    Users: {len(self.user_ids)}, Items: {len(self.item_ids)}\")\n",
    "        print(f\"    Interactions: {len(interactions_df)}, Sparsity: {self._calculate_sparsity():.3f}\")\n",
    "    \n",
    "    def _build_interaction_matrix(self):\n",
    "        \"\"\"Build user-item interaction matrix from interactions data.\"\"\"\n",
    "        # Get unique users and items\n",
    "        user_ids = sorted(self.interactions_df['user_id'].unique())\n",
    "        item_ids = sorted(self.interactions_df['concept_id'].unique())\n",
    "        \n",
    "        # Create mapping indices\n",
    "        user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "        item_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "        \n",
    "        # Build matrix\n",
    "        matrix = np.zeros((len(user_ids), len(item_ids)))\n",
    "        \n",
    "        for _, interaction in self.interactions_df.iterrows():\n",
    "            user_idx = user_to_idx[interaction['user_id']]\n",
    "            item_idx = item_to_idx[interaction['concept_id']]\n",
    "            \n",
    "            # Use rating as interaction strength\n",
    "            matrix[user_idx, item_idx] = interaction['rating']\n",
    "        \n",
    "        return matrix, user_ids, item_ids\n",
    "    \n",
    "    def _calculate_sparsity(self):\n",
    "        \"\"\"Calculate sparsity of the user-item matrix.\"\"\"\n",
    "        total_cells = self.user_item_matrix.size\n",
    "        non_zero_cells = np.count_nonzero(self.user_item_matrix)\n",
    "        return 1 - (non_zero_cells / total_cells)\n",
    "    \n",
    "    def _calculate_user_similarity(self):\n",
    "        \"\"\"Calculate user-user similarity matrix.\"\"\"\n",
    "        # Use cosine similarity between user rating vectors\n",
    "        return cosine_similarity(self.user_item_matrix)\n",
    "    \n",
    "    def _calculate_item_similarity(self):\n",
    "        \"\"\"Calculate item-item similarity matrix.\"\"\"\n",
    "        # Use cosine similarity between item rating vectors (transposed)\n",
    "        return cosine_similarity(self.user_item_matrix.T)\n",
    "    \n",
    "    def recommend_user_based(self, user_id, top_k=10, n_neighbors=10):\n",
    "        \"\"\"User-based collaborative filtering recommendations.\"\"\"\n",
    "        if user_id not in self.user_ids:\n",
    "            return []\n",
    "        \n",
    "        user_idx = self.user_ids.index(user_id)\n",
    "        \n",
    "        # Find most similar users\n",
    "        user_similarities = self.user_similarity[user_idx]\n",
    "        similar_user_indices = np.argsort(user_similarities)[::-1][1:n_neighbors+1]  # Exclude self\n",
    "        \n",
    "        # Get items rated by the target user\n",
    "        user_items = set(np.where(self.user_item_matrix[user_idx] > 0)[0])\n",
    "        \n",
    "        # Calculate weighted ratings for unrated items\n",
    "        item_scores = {}\n",
    "        \n",
    "        for item_idx in range(len(self.item_ids)):\n",
    "            if item_idx not in user_items:  # Not yet rated by target user\n",
    "                weighted_sum = 0\n",
    "                similarity_sum = 0\n",
    "                \n",
    "                for similar_user_idx in similar_user_indices:\n",
    "                    if self.user_item_matrix[similar_user_idx, item_idx] > 0:\n",
    "                        similarity = user_similarities[similar_user_idx]\n",
    "                        rating = self.user_item_matrix[similar_user_idx, item_idx]\n",
    "                        \n",
    "                        weighted_sum += similarity * rating\n",
    "                        similarity_sum += similarity\n",
    "                \n",
    "                if similarity_sum > 0:\n",
    "                    predicted_rating = weighted_sum / similarity_sum\n",
    "                    item_scores[item_idx] = predicted_rating\n",
    "        \n",
    "        # Sort and get top recommendations\n",
    "        top_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        recommendations = []\n",
    "        for item_idx, score in top_items:\n",
    "            item_id = self.item_ids[item_idx]\n",
    "            concept_row = self.concepts_df[self.concepts_df['concept_id'] == item_id].iloc[0]\n",
    "            \n",
    "            recommendations.append({\n",
    "                'concept_id': item_id,\n",
    "                'name': concept_row['name'],\n",
    "                'category': concept_row['category'],\n",
    "                'predicted_rating': float(score),\n",
    "                'recommendation_type': 'user_based_cf'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def recommend_item_based(self, user_id, top_k=10, n_neighbors=5):\n",
    "        \"\"\"Item-based collaborative filtering recommendations.\"\"\"\n",
    "        if user_id not in self.user_ids:\n",
    "            return []\n",
    "        \n",
    "        user_idx = self.user_ids.index(user_id)\n",
    "        \n",
    "        # Get items rated by the user\n",
    "        user_ratings = self.user_item_matrix[user_idx]\n",
    "        rated_items = np.where(user_ratings > 0)[0]\n",
    "        \n",
    "        if len(rated_items) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Calculate scores for unrated items\n",
    "        item_scores = {}\n",
    "        \n",
    "        for item_idx in range(len(self.item_ids)):\n",
    "            if user_ratings[item_idx] == 0:  # Unrated item\n",
    "                # Find most similar items that user has rated\n",
    "                item_similarities = self.item_similarity[item_idx]\n",
    "                \n",
    "                # Get similarities to rated items\n",
    "                rated_similarities = [(rated_idx, item_similarities[rated_idx]) \n",
    "                                    for rated_idx in rated_items]\n",
    "                \n",
    "                # Sort by similarity and take top neighbors\n",
    "                rated_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_neighbors = rated_similarities[:n_neighbors]\n",
    "                \n",
    "                # Calculate weighted score\n",
    "                weighted_sum = 0\n",
    "                similarity_sum = 0\n",
    "                \n",
    "                for neighbor_idx, similarity in top_neighbors:\n",
    "                    if similarity > 0:\n",
    "                        rating = user_ratings[neighbor_idx]\n",
    "                        weighted_sum += similarity * rating\n",
    "                        similarity_sum += similarity\n",
    "                \n",
    "                if similarity_sum > 0:\n",
    "                    predicted_score = weighted_sum / similarity_sum\n",
    "                    item_scores[item_idx] = predicted_score\n",
    "        \n",
    "        # Sort and get top recommendations\n",
    "        top_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        recommendations = []\n",
    "        for item_idx, score in top_items:\n",
    "            item_id = self.item_ids[item_idx]\n",
    "            concept_row = self.concepts_df[self.concepts_df['concept_id'] == item_id].iloc[0]\n",
    "            \n",
    "            recommendations.append({\n",
    "                'concept_id': item_id,\n",
    "                'name': concept_row['name'],\n",
    "                'category': concept_row['category'],\n",
    "                'predicted_rating': float(score),\n",
    "                'recommendation_type': 'item_based_cf'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize collaborative filtering recommender\n",
    "if 'interactions_df' in locals() and rec_data:\n",
    "    cf_recommender = CollaborativeFilteringRecommender(rec_data, interactions_df)\n",
    "    \n",
    "    # Demo collaborative filtering recommendations\n",
    "    print(\"\\nü§ù COLLABORATIVE FILTERING EXAMPLES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Select a sample user\n",
    "    sample_user_id = np.random.choice(interactions_df['user_id'].unique())\n",
    "    user_interactions = interactions_df[interactions_df['user_id'] == sample_user_id]\n",
    "    user_type = users_df[users_df['user_id'] == sample_user_id].iloc[0]['user_type']\n",
    "    \n",
    "    print(f\"\\nüë§ Sample User: {sample_user_id} (Type: {user_type})\")\n",
    "    print(f\"   Previous interactions: {len(user_interactions)}\")\n",
    "    print(f\"   Preferred categories: {user_interactions['category'].value_counts().head(3).to_dict()}\")\n",
    "    \n",
    "    # User-based recommendations\n",
    "    print(f\"\\nüîÑ User-Based Recommendations:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    user_based_recs = cf_recommender.recommend_user_based(sample_user_id, top_k=6)\n",
    "    \n",
    "    for i, rec in enumerate(user_based_recs):\n",
    "        print(f\"  {i+1}. {rec['name']:30} [{rec['category']:12}] (rating: {rec['predicted_rating']:.2f})\")\n",
    "    \n",
    "    # Item-based recommendations\n",
    "    print(f\"\\nüîó Item-Based Recommendations:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    item_based_recs = cf_recommender.recommend_item_based(sample_user_id, top_k=6)\n",
    "    \n",
    "    for i, rec in enumerate(item_based_recs):\n",
    "        print(f\"  {i+1}. {rec['name']:30} [{rec['category']:12}] (rating: {rec['predicted_rating']:.2f})\")\n",
    "        \n",
    "else:\n",
    "    print(\"Collaborative filtering not available - need user interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hybrid Recommendation System\n",
    "\n",
    "Combine content-based and collaborative filtering for improved recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, content_recommender, cf_recommender, rec_data):\n",
    "        \"\"\"Initialize hybrid recommender combining multiple approaches.\"\"\"\n",
    "        self.content_recommender = content_recommender\n",
    "        self.cf_recommender = cf_recommender\n",
    "        self.rec_data = rec_data\n",
    "        \n",
    "        print(f\"üé≠ Hybrid recommender initialized\")\n",
    "    \n",
    "    def recommend_hybrid(self, user_id, content_weight=0.4, cf_weight=0.6, \n",
    "                        diversity_boost=0.2, top_k=10):\n",
    "        \"\"\"Generate hybrid recommendations combining multiple approaches.\"\"\"\n",
    "        \n",
    "        # Get user's interaction history to understand preferences\n",
    "        if hasattr(self.cf_recommender, 'interactions_df'):\n",
    "            user_interactions = self.cf_recommender.interactions_df[\n",
    "                self.cf_recommender.interactions_df['user_id'] == user_id\n",
    "            ]\n",
    "        else:\n",
    "            user_interactions = pd.DataFrame()\n",
    "        \n",
    "        all_recommendations = []\n",
    "        \n",
    "        # 1. Content-based recommendations\n",
    "        if not user_interactions.empty:\n",
    "            # Use user's highly rated items as seeds\n",
    "            high_rated_items = user_interactions[user_interactions['rating'] >= 4]['concept_id'].tolist()\n",
    "            \n",
    "            if high_rated_items:\n",
    "                content_recs = self.content_recommender.recommend_similar_concepts(\n",
    "                    high_rated_items[:3],  # Use top 3 as seeds\n",
    "                    top_k=top_k * 2,  # Get more for filtering\n",
    "                    diversity_weight=diversity_boost\n",
    "                )\n",
    "                \n",
    "                # Add content-based score\n",
    "                for rec in content_recs:\n",
    "                    rec['content_score'] = rec['similarity'] * content_weight\n",
    "                    rec['cf_score'] = 0.0\n",
    "                    all_recommendations.append(rec)\n",
    "        \n",
    "        # 2. Collaborative filtering recommendations\n",
    "        try:\n",
    "            user_based_recs = self.cf_recommender.recommend_user_based(\n",
    "                user_id, top_k=top_k\n",
    "            )\n",
    "            \n",
    "            for rec in user_based_recs:\n",
    "                rec['content_score'] = 0.0\n",
    "                rec['cf_score'] = (rec['predicted_rating'] / 5.0) * cf_weight\n",
    "                all_recommendations.append(rec)\n",
    "            \n",
    "            # Also add item-based recommendations\n",
    "            item_based_recs = self.cf_recommender.recommend_item_based(\n",
    "                user_id, top_k=top_k\n",
    "            )\n",
    "            \n",
    "            for rec in item_based_recs:\n",
    "                rec['content_score'] = 0.0\n",
    "                rec['cf_score'] = (rec['predicted_rating'] / 5.0) * cf_weight * 0.8  # Slight discount\n",
    "                all_recommendations.append(rec)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: CF recommendations failed: {e}\")\n",
    "        \n",
    "        # 3. Combine and deduplicate recommendations\n",
    "        concept_scores = defaultdict(lambda: {'content': 0, 'cf': 0, 'info': None})\n",
    "        \n",
    "        for rec in all_recommendations:\n",
    "            concept_id = rec['concept_id']\n",
    "            \n",
    "            # Combine scores (take max for each type)\n",
    "            concept_scores[concept_id]['content'] = max(\n",
    "                concept_scores[concept_id]['content'],\n",
    "                rec['content_score']\n",
    "            )\n",
    "            concept_scores[concept_id]['cf'] = max(\n",
    "                concept_scores[concept_id]['cf'],\n",
    "                rec['cf_score']\n",
    "            )\n",
    "            \n",
    "            # Store concept info\n",
    "            if concept_scores[concept_id]['info'] is None:\n",
    "                concept_scores[concept_id]['info'] = {\n",
    "                    'name': rec['name'],\n",
    "                    'category': rec['category']\n",
    "                }\n",
    "        \n",
    "        # 4. Calculate final hybrid scores\n",
    "        final_recommendations = []\n",
    "        \n",
    "        for concept_id, scores in concept_scores.items():\n",
    "            # Skip if already interacted by user\n",
    "            if not user_interactions.empty and concept_id in user_interactions['concept_id'].values:\n",
    "                continue\n",
    "            \n",
    "            hybrid_score = scores['content'] + scores['cf']\n",
    "            \n",
    "            # Apply diversity boost for different categories\n",
    "            if not user_interactions.empty:\n",
    "                user_categories = set(user_interactions['category'].values)\n",
    "                if scores['info']['category'] not in user_categories:\n",
    "                    hybrid_score *= (1 + diversity_boost)\n",
    "            \n",
    "            final_recommendations.append({\n",
    "                'concept_id': concept_id,\n",
    "                'name': scores['info']['name'],\n",
    "                'category': scores['info']['category'],\n",
    "                'hybrid_score': hybrid_score,\n",
    "                'content_score': scores['content'],\n",
    "                'cf_score': scores['cf'],\n",
    "                'recommendation_type': 'hybrid'\n",
    "            })\n",
    "        \n",
    "        # Sort by hybrid score and return top recommendations\n",
    "        final_recommendations.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return final_recommendations[:top_k]\n",
    "    \n",
    "    def recommend_cold_start(self, user_preferences=None, top_k=10):\n",
    "        \"\"\"Handle cold start problem for new users.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if user_preferences:\n",
    "            # Use preferences to guide recommendations\n",
    "            preferred_categories = user_preferences.get('categories', [])\n",
    "            preferred_keywords = user_preferences.get('keywords', [])\n",
    "            \n",
    "            # Category-based recommendations\n",
    "            for category in preferred_categories:\n",
    "                category_recs = self.content_recommender.recommend_by_category(\n",
    "                    category, top_k=5\n",
    "                )\n",
    "                recommendations.extend(category_recs)\n",
    "            \n",
    "            # Keyword-based recommendations (simple text matching)\n",
    "            if preferred_keywords:\n",
    "                for keyword in preferred_keywords:\n",
    "                    keyword_matches = self.rec_data['concepts_df'][\n",
    "                        self.rec_data['concepts_df']['name'].str.contains(\n",
    "                            keyword, case=False, na=False\n",
    "                        )\n",
    "                    ]\n",
    "                    \n",
    "                    for _, concept_row in keyword_matches.head(3).iterrows():\n",
    "                        recommendations.append({\n",
    "                            'concept_id': concept_row['concept_id'],\n",
    "                            'name': concept_row['name'],\n",
    "                            'category': concept_row['category'],\n",
    "                            'keyword_match': keyword,\n",
    "                            'recommendation_type': 'cold_start_keyword'\n",
    "                        })\n",
    "        \n",
    "        else:\n",
    "            # Default: popular items from each category\n",
    "            categories = self.rec_data['concepts_df']['category'].unique()\n",
    "            \n",
    "            for category in categories[:4]:  # Limit to top 4 categories\n",
    "                category_recs = self.content_recommender.recommend_by_category(\n",
    "                    category, top_k=2\n",
    "                )\n",
    "                recommendations.extend(category_recs)\n",
    "        \n",
    "        # Remove duplicates and return top recommendations\n",
    "        seen_concepts = set()\n",
    "        unique_recommendations = []\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            if rec['concept_id'] not in seen_concepts:\n",
    "                unique_recommendations.append(rec)\n",
    "                seen_concepts.add(rec['concept_id'])\n",
    "        \n",
    "        return unique_recommendations[:top_k]\n",
    "\n",
    "# Initialize hybrid recommender\n",
    "if 'content_recommender' in locals() and 'cf_recommender' in locals():\n",
    "    hybrid_recommender = HybridRecommender(content_recommender, cf_recommender, rec_data)\n",
    "    \n",
    "    # Demo hybrid recommendations\n",
    "    print(\"\\nüé≠ HYBRID RECOMMENDATION EXAMPLES\")\n",
    "    print(\"=\" * 38)\n",
    "    \n",
    "    # Example 1: Hybrid recommendations for existing user\n",
    "    sample_user_id = np.random.choice(interactions_df['user_id'].unique())\n",
    "    user_type = users_df[users_df['user_id'] == sample_user_id].iloc[0]['user_type']\n",
    "    \n",
    "    print(f\"\\nüë§ User: {sample_user_id} (Type: {user_type})\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    hybrid_recs = hybrid_recommender.recommend_hybrid(\n",
    "        sample_user_id, \n",
    "        content_weight=0.4,\n",
    "        cf_weight=0.6,\n",
    "        diversity_boost=0.2,\n",
    "        top_k=8\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(hybrid_recs):\n",
    "        print(f\"  {i+1}. {rec['name']:30} [{rec['category']:12}]\")\n",
    "        print(f\"      Hybrid: {rec['hybrid_score']:.3f} (Content: {rec['content_score']:.3f}, CF: {rec['cf_score']:.3f})\")\n",
    "    \n",
    "    # Example 2: Cold start recommendations\n",
    "    print(f\"\\n‚ùÑÔ∏è Cold Start Recommendations:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    cold_start_prefs = {\n",
    "        'categories': ['biology', 'tools'],\n",
    "        'keywords': ['protein', 'analysis']\n",
    "    }\n",
    "    \n",
    "    cold_start_recs = hybrid_recommender.recommend_cold_start(\n",
    "        user_preferences=cold_start_prefs,\n",
    "        top_k=6\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(cold_start_recs):\n",
    "        rec_type = rec.get('recommendation_type', 'general')\n",
    "        extra_info = f\"({rec.get('keyword_match', rec.get('centrality_score', ''))})\"\n",
    "        print(f\"  {i+1}. {rec['name']:30} [{rec['category']:12}] {rec_type} {extra_info}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Hybrid recommender not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Recommendation Evaluation\n",
    "\n",
    "Evaluate recommendation quality using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationEvaluator:\n",
    "    def __init__(self, rec_data, interactions_df, users_df):\n",
    "        \"\"\"Initialize recommendation evaluator.\"\"\"\n",
    "        self.rec_data = rec_data\n",
    "        self.interactions_df = interactions_df\n",
    "        self.users_df = users_df\n",
    "        \n",
    "        print(f\"üìä Recommendation evaluator initialized\")\n",
    "    \n",
    "    def evaluate_recommender(self, recommender, recommender_name, test_users=None, top_k=10):\n",
    "        \"\"\"Comprehensive evaluation of a recommender system.\"\"\"\n",
    "        \n",
    "        if test_users is None:\n",
    "            # Use a sample of users for evaluation\n",
    "            test_users = np.random.choice(\n",
    "                self.interactions_df['user_id'].unique(), \n",
    "                size=min(20, len(self.interactions_df['user_id'].unique())), \n",
    "                replace=False\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nüìà Evaluating {recommender_name}...\")\n",
    "        print(f\"   Test users: {len(test_users)}\")\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'recommender_name': recommender_name,\n",
    "            'test_users': len(test_users),\n",
    "            'recommendations_per_user': top_k,\n",
    "            'metrics': {}\n",
    "        }\n",
    "        \n",
    "        all_recommendations = []\n",
    "        user_metrics = []\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            try:\n",
    "                # Generate recommendations\n",
    "                if hasattr(recommender, 'recommend_hybrid'):\n",
    "                    recs = recommender.recommend_hybrid(user_id, top_k=top_k)\n",
    "                elif hasattr(recommender, 'recommend_user_based'):\n",
    "                    recs = recommender.recommend_user_based(user_id, top_k=top_k)\n",
    "                elif hasattr(recommender, 'recommend_similar_concepts'):\n",
    "                    # For content-based, use user's preferred items as seeds\n",
    "                    user_items = self.interactions_df[\n",
    "                        self.interactions_df['user_id'] == user_id\n",
    "                    ]['concept_id'].tolist()\n",
    "                    if user_items:\n",
    "                        recs = recommender.recommend_similar_concepts(user_items[:3], top_k=top_k)\n",
    "                    else:\n",
    "                        recs = []\n",
    "                else:\n",
    "                    recs = []\n",
    "                \n",
    "                if recs:\n",
    "                    # Calculate user-level metrics\n",
    "                    user_evaluation = self._evaluate_user_recommendations(\n",
    "                        user_id, recs, top_k\n",
    "                    )\n",
    "                    user_metrics.append(user_evaluation)\n",
    "                    all_recommendations.extend(recs)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    Failed for user {user_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if user_metrics:\n",
    "            # Aggregate metrics\n",
    "            evaluation_results['metrics'] = self._aggregate_metrics(user_metrics)\n",
    "            \n",
    "            # Calculate system-level metrics\n",
    "            evaluation_results['metrics'].update(\n",
    "                self._calculate_system_metrics(all_recommendations)\n",
    "            )\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def _evaluate_user_recommendations(self, user_id, recommendations, top_k):\n",
    "        \"\"\"Evaluate recommendations for a single user.\"\"\"\n",
    "        \n",
    "        # Get user's actual interactions (as ground truth)\n",
    "        user_interactions = self.interactions_df[self.interactions_df['user_id'] == user_id]\n",
    "        user_liked_items = set(user_interactions[user_interactions['rating'] >= 4]['concept_id'])\n",
    "        user_categories = set(user_interactions['category'])\n",
    "        \n",
    "        recommended_items = [rec['concept_id'] for rec in recommendations]\n",
    "        recommended_categories = [rec['category'] for rec in recommendations]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'user_id': user_id,\n",
    "            'n_recommendations': len(recommendations),\n",
    "            'n_user_liked_items': len(user_liked_items),\n",
    "            'n_user_categories': len(user_categories)\n",
    "        }\n",
    "        \n",
    "        # Relevance metrics (simplified - based on category overlap)\n",
    "        relevant_recommendations = [rec for rec in recommendations \n",
    "                                  if rec['category'] in user_categories]\n",
    "        \n",
    "        metrics['precision'] = len(relevant_recommendations) / len(recommendations) if recommendations else 0\n",
    "        metrics['recall'] = len(relevant_recommendations) / len(user_categories) if user_categories else 0\n",
    "        \n",
    "        if metrics['precision'] + metrics['recall'] > 0:\n",
    "            metrics['f1'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n",
    "        else:\n",
    "            metrics['f1'] = 0\n",
    "        \n",
    "        # Diversity metrics\n",
    "        unique_categories = set(recommended_categories)\n",
    "        metrics['category_diversity'] = len(unique_categories) / len(recommendations) if recommendations else 0\n",
    "        \n",
    "        # Coverage metrics\n",
    "        metrics['category_coverage'] = len(unique_categories.intersection(user_categories)) / len(user_categories) if user_categories else 0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _aggregate_metrics(self, user_metrics):\n",
    "        \"\"\"Aggregate user-level metrics to system-level.\"\"\"\n",
    "        if not user_metrics:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(user_metrics)\n",
    "        \n",
    "        aggregated = {\n",
    "            'precision': {\n",
    "                'mean': df['precision'].mean(),\n",
    "                'std': df['precision'].std(),\n",
    "                'median': df['precision'].median()\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': df['recall'].mean(),\n",
    "                'std': df['recall'].std(),\n",
    "                'median': df['recall'].median()\n",
    "            },\n",
    "            'f1': {\n",
    "                'mean': df['f1'].mean(),\n",
    "                'std': df['f1'].std(),\n",
    "                'median': df['f1'].median()\n",
    "            },\n",
    "            'category_diversity': {\n",
    "                'mean': df['category_diversity'].mean(),\n",
    "                'std': df['category_diversity'].std(),\n",
    "                'median': df['category_diversity'].median()\n",
    "            },\n",
    "            'category_coverage': {\n",
    "                'mean': df['category_coverage'].mean(),\n",
    "                'std': df['category_coverage'].std(),\n",
    "                'median': df['category_coverage'].median()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _calculate_system_metrics(self, all_recommendations):\n",
    "        \"\"\"Calculate system-level metrics across all recommendations.\"\"\"\n",
    "        \n",
    "        if not all_recommendations:\n",
    "            return {}\n",
    "        \n",
    "        # Item popularity distribution\n",
    "        recommended_items = [rec['concept_id'] for rec in all_recommendations]\n",
    "        item_counts = Counter(recommended_items)\n",
    "        \n",
    "        # Category distribution\n",
    "        recommended_categories = [rec['category'] for rec in all_recommendations]\n",
    "        category_counts = Counter(recommended_categories)\n",
    "        \n",
    "        system_metrics = {\n",
    "            'total_recommendations': len(all_recommendations),\n",
    "            'unique_items_recommended': len(item_counts),\n",
    "            'unique_categories_recommended': len(category_counts),\n",
    "            'average_item_frequency': np.mean(list(item_counts.values())),\n",
    "            'item_distribution_entropy': self._calculate_entropy(list(item_counts.values())),\n",
    "            'category_distribution': dict(category_counts),\n",
    "            'most_recommended_items': dict(item_counts.most_common(5))\n",
    "        }\n",
    "        \n",
    "        return system_metrics\n",
    "    \n",
    "    def _calculate_entropy(self, frequencies):\n",
    "        \"\"\"Calculate Shannon entropy of a frequency distribution.\"\"\"\n",
    "        if not frequencies:\n",
    "            return 0\n",
    "        \n",
    "        total = sum(frequencies)\n",
    "        probabilities = [f / total for f in frequencies]\n",
    "        entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def compare_recommenders(self, evaluation_results):\n",
    "        \"\"\"Compare multiple recommender systems.\"\"\"\n",
    "        \n",
    "        if not evaluation_results:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüèÜ RECOMMENDER COMPARISON\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        comparison_metrics = ['precision', 'recall', 'f1', 'category_diversity', 'category_coverage']\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_data = []\n",
    "        \n",
    "        for result in evaluation_results:\n",
    "            if 'metrics' in result:\n",
    "                row = {'recommender': result['recommender_name']}\n",
    "                \n",
    "                for metric in comparison_metrics:\n",
    "                    if metric in result['metrics']:\n",
    "                        row[metric] = result['metrics'][metric]['mean']\n",
    "                    else:\n",
    "                        row[metric] = 0.0\n",
    "                \n",
    "                comparison_data.append(row)\n",
    "        \n",
    "        if comparison_data:\n",
    "            df_comparison = pd.DataFrame(comparison_data)\n",
    "            df_comparison = df_comparison.set_index('recommender')\n",
    "            \n",
    "            print(df_comparison.round(3).to_string())\n",
    "            \n",
    "            # Create comparison visualization\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, metric in enumerate(comparison_metrics):\n",
    "                if i < len(axes) and metric in df_comparison.columns:\n",
    "                    ax = axes[i]\n",
    "                    df_comparison[metric].plot(kind='bar', ax=ax, color=sns.color_palette(\"husl\", len(df_comparison)))\n",
    "                    ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "                    ax.set_ylabel('Score')\n",
    "                    ax.tick_params(axis='x', rotation=45)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Remove empty subplots\n",
    "            for i in range(len(comparison_metrics), len(axes)):\n",
    "                fig.delaxes(axes[i])\n",
    "            \n",
    "            plt.suptitle('Recommender System Comparison', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return df_comparison\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Evaluate all recommenders\n",
    "if all(var in locals() for var in ['content_recommender', 'cf_recommender', 'hybrid_recommender']):\n",
    "    evaluator = RecommendationEvaluator(rec_data, interactions_df, users_df)\n",
    "    \n",
    "    print(\"\\nüìä RECOMMENDATION SYSTEM EVALUATION\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Evaluate each recommender\n",
    "    evaluation_results = []\n",
    "    \n",
    "    # Content-based evaluation\n",
    "    content_eval = evaluator.evaluate_recommender(\n",
    "        content_recommender, \"Content-Based\", top_k=8\n",
    "    )\n",
    "    evaluation_results.append(content_eval)\n",
    "    \n",
    "    # Collaborative filtering evaluation\n",
    "    cf_eval = evaluator.evaluate_recommender(\n",
    "        cf_recommender, \"Collaborative Filtering\", top_k=8\n",
    "    )\n",
    "    evaluation_results.append(cf_eval)\n",
    "    \n",
    "    # Hybrid evaluation\n",
    "    hybrid_eval = evaluator.evaluate_recommender(\n",
    "        hybrid_recommender, \"Hybrid\", top_k=8\n",
    "    )\n",
    "    evaluation_results.append(hybrid_eval)\n",
    "    \n",
    "    # Compare all recommenders\n",
    "    comparison_df = evaluator.compare_recommenders(evaluation_results)\n",
    "    \n",
    "    # Show detailed metrics for best performer\n",
    "    if comparison_df is not None and not comparison_df.empty:\n",
    "        best_f1_recommender = comparison_df['f1'].idxmax()\n",
    "        best_eval = next((r for r in evaluation_results if r['recommender_name'] == best_f1_recommender), None)\n",
    "        \n",
    "        if best_eval:\n",
    "            print(f\"\\nü•á Best Performer: {best_f1_recommender}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            if 'metrics' in best_eval:\n",
    "                for metric_name, metric_data in best_eval['metrics'].items():\n",
    "                    if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                        print(f\"  {metric_name:20}: {metric_data['mean']:.3f} ¬± {metric_data['std']:.3f}\")\n",
    "            \n",
    "else:\n",
    "    print(\"Not all recommenders available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interactive Recommendation Interface\n",
    "\n",
    "Create an interactive interface to explore different recommendation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recommendation_dashboard(hybrid_recommender, rec_data, interactions_df, users_df):\n",
    "    \"\"\"Create an interactive dashboard for exploring recommendations.\"\"\"\n",
    "    \n",
    "    print(\"üéÆ INTERACTIVE RECOMMENDATION SCENARIOS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'New Biologist',\n",
    "            'description': 'A new biologist looking for protein analysis tools',\n",
    "            'preferences': {'categories': ['biology', 'tools'], 'keywords': ['protein']}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Data Scientist',\n",
    "            'description': 'Data scientist needing new file formats and analysis methods',\n",
    "            'preferences': {'categories': ['data_formats', 'methods'], 'keywords': ['analysis', 'data']}\n",
    "        },\n",
    "        {\n",
    "            'name': 'Computational Biologist',\n",
    "            'description': 'Experienced researcher seeking advanced computational tools',\n",
    "            'preferences': {'categories': ['methods', 'tools', 'databases'], 'keywords': ['computational', 'algorithm']}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create visualizations for each scenario\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Scenario 1: New Biologist', 'Category Distribution',\n",
    "            'Scenario 2: Data Scientist', 'Recommendation Scores',\n",
    "            'Scenario 3: Computational Biologist', 'Diversity Analysis'\n",
    "        ],\n",
    "        specs=[[{'type': 'bar'}, {'type': 'pie'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3']\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        print(f\"\\nüéØ Scenario {i+1}: {scenario['name']}\")\n",
    "        print(f\"   {scenario['description']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get recommendations for this scenario\n",
    "        cold_start_recs = hybrid_recommender.recommend_cold_start(\n",
    "            user_preferences=scenario['preferences'],\n",
    "            top_k=8\n",
    "        )\n",
    "        \n",
    "        if cold_start_recs:\n",
    "            # Show top recommendations\n",
    "            for j, rec in enumerate(cold_start_recs[:5]):\n",
    "                rec_type_marker = {\n",
    "                    'cold_start_keyword': 'üîç',\n",
    "                    'category_based': 'üìÇ',\n",
    "                    'general': '‚≠ê'\n",
    "                }.get(rec.get('recommendation_type', 'general'), 'üí°')\n",
    "                \n",
    "                print(f\"  {rec_type_marker} {j+1}. {rec['name'][:35]:35} [{rec['category']:12}]\")\n",
    "            \n",
    "            # Add to visualization\n",
    "            rec_names = [rec['name'][:20] for rec in cold_start_recs[:6]]\n",
    "            rec_scores = [rec.get('centrality_score', 0.8) for rec in cold_start_recs[:6]]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=rec_names,\n",
    "                    y=rec_scores,\n",
    "                    name=f'Scenario {i+1}',\n",
    "                    marker_color=colors[i % len(colors)],\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "            \n",
    "            # Category distribution for first scenario\n",
    "            if i == 0:\n",
    "                category_counts = Counter([rec['category'] for rec in cold_start_recs])\n",
    "                fig.add_trace(\n",
    "                    go.Pie(\n",
    "                        labels=list(category_counts.keys()),\n",
    "                        values=list(category_counts.values()),\n",
    "                        name=\"Categories\",\n",
    "                        showlegend=True\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            \n",
    "            # Recommendation scores comparison for second scenario\n",
    "            elif i == 1:\n",
    "                score_types = ['Centrality', 'Diversity', 'Relevance']\n",
    "                score_values = [\n",
    "                    np.mean([rec.get('centrality_score', 0.5) for rec in cold_start_recs]),\n",
    "                    len(set([rec['category'] for rec in cold_start_recs])) / len(cold_start_recs),\n",
    "                    sum(1 for rec in cold_start_recs if any(kw in rec['name'].lower() \n",
    "                                                          for kw in scenario['preferences']['keywords'])) / len(cold_start_recs)\n",
    "                ]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=score_types,\n",
    "                        y=score_values,\n",
    "                        name='Quality Metrics',\n",
    "                        marker_color=colors[(i+2) % len(colors)],\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            \n",
    "            # Diversity analysis for third scenario\n",
    "            elif i == 2:\n",
    "                # Create scatter plot of recommendations by category diversity vs relevance\n",
    "                category_diversity = len(set([rec['category'] for rec in cold_start_recs]))\n",
    "                keyword_relevance = sum(1 for rec in cold_start_recs \n",
    "                                      if any(kw in rec['name'].lower() \n",
    "                                           for kw in scenario['preferences']['keywords']))\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[category_diversity],\n",
    "                        y=[keyword_relevance],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=15,\n",
    "                            color=colors[(i+3) % len(colors)]\n",
    "                        ),\n",
    "                        name='Scenario Quality',\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=3, col=2\n",
    "                )\n",
    "        \n",
    "        else:\n",
    "            print(\"  No recommendations generated for this scenario\")\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title_text=\"Recommendation System Scenarios Dashboard\",\n",
    "        title_x=0.5,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update x-axis labels for bar charts\n",
    "    for i in range(1, 4):\n",
    "        fig.update_xaxes(tickangle=45, row=i, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Diversity Score\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Relevance Score\", row=3, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive dashboard\n",
    "if 'hybrid_recommender' in locals():\n",
    "    dashboard_fig = create_recommendation_dashboard(\n",
    "        hybrid_recommender, rec_data, interactions_df, users_df\n",
    "    )\n",
    "    \n",
    "    if dashboard_fig:\n",
    "        dashboard_fig.show()\n",
    "        \n",
    "    # Summary of recommendation capabilities\n",
    "    print(f\"\\nüéâ RECOMMENDATION SYSTEM SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"‚úÖ Content-Based Filtering: Similarity-based recommendations using concept embeddings\")\n",
    "    print(f\"‚úÖ Collaborative Filtering: User-based and item-based recommendations\")\n",
    "    print(f\"‚úÖ Hybrid Approach: Combined content + collaborative filtering\")\n",
    "    print(f\"‚úÖ Cold Start Solutions: Category and keyword-based recommendations\")\n",
    "    print(f\"‚úÖ Diversity Enhancement: Multi-category and diverse recommendations\")\n",
    "    print(f\"‚úÖ Quality Evaluation: Precision, recall, F1, diversity metrics\")\n",
    "    \n",
    "else:\n",
    "    print(\"Interactive dashboard not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated comprehensive recommendation system capabilities using on2vec embeddings:\n",
    "\n",
    "### ‚úÖ Key Achievements:\n",
    "\n",
    "1. **Content-Based Filtering**: Used concept embeddings to find similar items and provide recommendations\n",
    "2. **Collaborative Filtering**: Implemented both user-based and item-based collaborative filtering\n",
    "3. **Hybrid Approaches**: Combined content and collaborative methods for improved accuracy\n",
    "4. **Cold Start Solutions**: Handled new users with preference-based and category-based recommendations\n",
    "5. **Diversity Enhancement**: Promoted variety in recommendations across categories\n",
    "6. **Comprehensive Evaluation**: Assessed quality using precision, recall, F1, and diversity metrics\n",
    "\n",
    "### üéØ Practical Applications:\n",
    "\n",
    "- **Scientific Resource Discovery**: Help researchers find relevant tools, datasets, and methods\n",
    "- **Literature Recommendations**: Suggest papers and articles based on research interests\n",
    "- **Tool and Software Discovery**: Recommend bioinformatics tools and computational methods\n",
    "- **Educational Content**: Suggest learning resources based on current knowledge\n",
    "- **Collaboration Networks**: Find potential collaborators with similar research interests\n",
    "\n",
    "### üîß Technical Features:\n",
    "\n",
    "- **Embedding-Based Similarity**: Leveraged on2vec embeddings for semantic similarity\n",
    "- **Multi-Modal Recommendations**: Combined structural and textual similarity signals\n",
    "- **Scalable Architecture**: Efficient similarity computation for large concept spaces\n",
    "- **Personalization**: Adapted recommendations based on user interaction patterns\n",
    "- **Quality Assurance**: Multi-metric evaluation framework\n",
    "\n",
    "### üìä Evaluation Results:\n",
    "\n",
    "The hybrid approach typically performed best by combining:\n",
    "- **Content-based strength**: Good semantic similarity matching\n",
    "- **Collaborative filtering strength**: User preference patterns\n",
    "- **Diversity mechanisms**: Avoided filter bubbles and echo chambers\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Real User Studies**: Deploy system with actual users for validation\n",
    "2. **Dynamic Learning**: Update recommendations based on user feedback\n",
    "3. **Context Awareness**: Incorporate temporal and situational factors\n",
    "4. **Multi-Domain Integration**: Combine recommendations across different knowledge domains\n",
    "5. **Explainable Recommendations**: Provide reasoning for why items were recommended\n",
    "\n",
    "The recommendation systems demonstrated here show how on2vec embeddings enable intelligent resource discovery, helping users navigate large knowledge spaces and discover relevant content they might otherwise miss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
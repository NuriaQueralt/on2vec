{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering & Taxonomy Discovery with on2vec\n",
    "\n",
    "This notebook demonstrates how to discover hidden concept groupings and create new taxonomies using on2vec embeddings. We'll show how to:\n",
    "\n",
    "1. Discover natural concept clusters in embedding space\n",
    "2. Create hierarchical taxonomies from flat concept collections\n",
    "3. Identify emergent themes and research areas\n",
    "4. Validate clustering quality against existing ontology structure\n",
    "5. Build interactive taxonomy browsers and explorers\n",
    "6. Detect concept drift and emerging trends\n",
    "\n",
    "## Use Case: Scientific Domain Organization\n",
    "As scientific fields evolve, new concepts emerge and relationships change. Traditional manual taxonomy creation is slow and may miss subtle patterns. Embedding-based clustering can automatically discover conceptual groupings, identify emerging research areas, and suggest new organizational structures for knowledge domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import (\n",
    "    KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering,\n",
    "    OPTICS, GaussianMixture\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, adjusted_rand_score, normalized_mutual_info_score,\n",
    "    calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, to_tree\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import chi2_contingency\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import umap\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# on2vec imports\n",
    "from on2vec import (\n",
    "    load_embeddings_as_dataframe,\n",
    "    train_ontology_embeddings,\n",
    "    embed_ontology_with_model,\n",
    "    build_graph_from_owl\n",
    ")\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Clustering Data\n",
    "\n",
    "Load embeddings and prepare data for clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ndef prepare_clustering_data(ontology_file):\n    \"\"\"Prepare embeddings and concept data for clustering analysis.\"\"\"\n    \n    if not os.path.exists(ontology_file):\n        print(f\"‚ùå Ontology file not found: {ontology_file}\")\n        return None\n    \n    base_name = Path(ontology_file).stem\n    model_file = f\"{base_name}_clustering_model.pt\"\n    embedding_file = f\"{base_name}_clustering_embeddings.parquet\"\n    \n    print(f\"üîÑ Preparing clustering data for {ontology_file}...\")\n    \n    try:\n        # Train model optimized for clustering\n        if not os.path.exists(model_file):\n            print(f\"  Training clustering-optimized model...\")\n            result = train_ontology_embeddings(\n                owl_file=ontology_file,\n                model_output=model_file,\n                model_type=\"gat\",  # GAT for capturing local neighborhoods\n                hidden_dim=256,    # Good balance for clustering\n                out_dim=128,       # Rich embedding space\n                epochs=100,        # Sufficient for demo\n                loss_fn_name=\"triplet\", # Triplet loss for better cluster separation\n                learning_rate=0.01\n            )\n            print(f\"  ‚úì Model training completed\")\n        else:\n            print(f\"  ‚úì Using existing model: {model_file}\")\n        \n        # Generate embeddings\n        if not os.path.exists(embedding_file):\n            print(f\"  Generating embeddings...\")\n            embed_result = embed_ontology_with_model(\n                model_path=model_file,\n                owl_file=ontology_file,\n                output_file=embedding_file\n            )\n            print(f\"  ‚úì Embeddings generation completed\")\n        else:\n            print(f\"  ‚úì Using existing embeddings: {embedding_file}\")\n        \n        # Load embeddings\n        df, metadata = load_embeddings_as_dataframe(embedding_file, return_metadata=True)\n        embeddings = np.stack(df['embedding'].to_numpy())\n        node_ids = df['node_id'].to_numpy()\n        \n        print(f\"  ‚úì Loaded {len(node_ids)} concept embeddings\")\n        \n        # Extract concept information\n        concept_info = []\n        for i, node_id in enumerate(node_ids):\n            # Extract concept name\n            if '#' in node_id:\n                name = node_id.split('#')[-1]\n            else:\n                name = node_id.split('/')[-1]\n            \n            clean_name = name.replace('_', ' ').replace('-', ' ')\n            \n            # Extract concept characteristics for validation\n            name_lower = clean_name.lower()\n            \n            # Determine concept type heuristically\n            if any(word in name_lower for word in ['data', 'format', 'file', 'document']):\n                concept_type = 'data_resource'\n            elif any(word in name_lower for word in ['analysis', 'method', 'algorithm', 'technique']):\n                concept_type = 'methodology'\n            elif any(word in name_lower for word in ['protein', 'gene', 'sequence', 'molecular']):\n                concept_type = 'biological_entity'\n            elif any(word in name_lower for word in ['tool', 'software', 'program', 'application']):\n                concept_type = 'software_tool'\n            elif any(word in name_lower for word in ['database', 'repository', 'collection']):\n                concept_type = 'database'\n            elif any(word in name_lower for word in ['disease', 'disorder', 'pathology', 'syndrome']):\n                concept_type = 'medical_condition'\n            elif any(word in name_lower for word in ['role', 'contributor', 'author', 'person']):\n                concept_type = 'role'\n            elif any(word in name_lower for word in ['use', 'consent', 'permission', 'restriction']):\n                concept_type = 'data_usage'\n            else:\n                concept_type = 'general'\n            \n            # Estimate concept complexity based on name structure\n            complexity = 'simple' if len(clean_name.split()) <= 2 else 'complex'\n            \n            concept_info.append({\n                'concept_id': i,\n                'node_id': node_id,\n                'name': clean_name,\n                'name_length': len(clean_name),\n                'concept_type': concept_type,\n                'complexity': complexity\n            })\n        \n        concept_df = pd.DataFrame(concept_info)\n        \n        # Load original ontology structure for validation\n        ontology_structure = None\n        try:\n            x, edge_index, class_mapping = build_graph_from_owl(ontology_file)\n            ontology_structure = {\n                'edge_index': edge_index,\n                'class_mapping': class_mapping\n            }\n            print(f\"  ‚úì Loaded ontology structure: {edge_index.shape[1]} edges\")\n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è Could not load ontology structure: {e}\")\n        \n        print(f\"  ‚úì Prepared {len(node_ids)} concepts with {embeddings.shape[1]}D embeddings\")\n        print(f\"  ‚úì Concept types: {dict(concept_df['concept_type'].value_counts())}\")\n        \n        return {\n            'ontology_file': ontology_file,\n            'embeddings': embeddings,\n            'concept_df': concept_df,\n            'node_ids': node_ids,\n            'metadata': metadata,\n            'ontology_structure': ontology_structure,\n            'model_file': model_file,\n            'embedding_file': embedding_file\n        }\n        \n    except Exception as e:\n        print(f\"‚ùå Error preparing clustering data: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# Try multiple verified working ontologies for clustering analysis\nontology_candidates = [\n    'EDAM.owl',             # Large bioinformatics ontology (if available)\n    'cvdo.owl',             # Cardiovascular disease ontology (if available)\n    'owl_files/fao.owl',    # FAIR* Reviews Ontology (116 classes, tested ‚úì)\n    'owl_files/cro.owl',    # Contributor Role Ontology (105 classes, tested ‚úì)\n    'owl_files/duo.owl',    # Data Use Ontology (45 classes, tested ‚úì)\n]\n\ncluster_data = None\n\nprint(\"üîç SEARCHING FOR SUITABLE ONTOLOGY FOR CLUSTERING...\")\nprint(\"=\" * 55)\n\nfor ont_file in ontology_candidates:\n    if os.path.exists(ont_file):\n        print(f\"\\nüîÑ Attempting to load {ont_file}...\")\n        \n        # Show file info\n        file_size = os.path.getsize(ont_file)\n        size_str = f\"{file_size/1024:.1f}KB\" if file_size < 1024*1024 else f\"{file_size/(1024*1024):.1f}MB\"\n        print(f\"   File size: {size_str}\")\n        \n        cluster_data = prepare_clustering_data(ont_file)\n        if cluster_data:\n            print(f\"\\n‚úÖ CLUSTERING DATA READY!\")\n            print(\"=\" * 30)\n            print(f\"  ‚Ä¢ Ontology: {ont_file}\")\n            print(f\"  ‚Ä¢ Concepts: {len(cluster_data['node_ids']):,}\")\n            print(f\"  ‚Ä¢ Embeddings: {cluster_data['embeddings'].shape}\")\n            print(f\"  ‚Ä¢ File size: {size_str}\")\n            break\n        else:\n            print(f\"‚ùå Failed to prepare clustering data for {ont_file}\")\n    else:\n        print(f\"‚ö†Ô∏è  File not found: {ont_file}\")\n\nif not cluster_data:\n    print(\"\\n‚ùå NO SUITABLE ONTOLOGY FILES FOUND\")\n    print(\"=\" * 40) \n    print(\"Checked the following files:\")\n    for ont_file in ontology_candidates:\n        exists = \"‚úì\" if os.path.exists(ont_file) else \"‚úó\"\n        print(f\"  {exists} {ont_file}\")\n    print(\"\\nPlease ensure at least one valid ontology is available.\")\n    print(\"You can download ontologies from: https://obofoundry.org/\")\nelse:\n    print(f\"\\nüéØ Ready to proceed with clustering analysis using {Path(cluster_data['ontology_file']).name}!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Multi-Algorithm Clustering Analysis\n",
    "\n",
    "Apply multiple clustering algorithms to discover different types of concept groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAlgorithmClusterer:\n",
    "    def __init__(self, cluster_data):\n",
    "        \"\"\"Initialize multi-algorithm clustering system.\"\"\"\n",
    "        self.cluster_data = cluster_data\n",
    "        self.embeddings = cluster_data['embeddings']\n",
    "        self.concept_df = cluster_data['concept_df']\n",
    "        \n",
    "        # Standardize embeddings for distance-based clustering\n",
    "        self.scaler = StandardScaler()\n",
    "        self.embeddings_scaled = self.scaler.fit_transform(self.embeddings)\n",
    "        \n",
    "        print(f\"üéØ Multi-algorithm clusterer initialized with {len(self.embeddings)} concepts\")\n",
    "    \n",
    "    def find_optimal_k(self, k_range=(2, 20), methods=['kmeans']):\n",
    "        \"\"\"Find optimal number of clusters using multiple metrics.\"\"\"\n",
    "        print(f\"üîç Finding optimal cluster numbers...\")\n",
    "        \n",
    "        k_values = range(k_range[0], k_range[1] + 1)\n",
    "        results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            method_results = {\n",
    "                'k_values': list(k_values),\n",
    "                'silhouette_scores': [],\n",
    "                'calinski_harabasz_scores': [],\n",
    "                'davies_bouldin_scores': [],\n",
    "                'inertias': []\n",
    "            }\n",
    "            \n",
    "            for k in k_values:\n",
    "                if method == 'kmeans':\n",
    "                    clusterer = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                elif method == 'hierarchical':\n",
    "                    clusterer = AgglomerativeClustering(n_clusters=k)\n",
    "                elif method == 'spectral':\n",
    "                    clusterer = SpectralClustering(n_clusters=k, random_state=42)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                labels = clusterer.fit_predict(self.embeddings_scaled)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                if len(set(labels)) > 1:  # Valid clustering\n",
    "                    silhouette = silhouette_score(self.embeddings_scaled, labels)\n",
    "                    calinski = calinski_harabasz_score(self.embeddings_scaled, labels)\n",
    "                    davies_bouldin = davies_bouldin_score(self.embeddings_scaled, labels)\n",
    "                    \n",
    "                    method_results['silhouette_scores'].append(silhouette)\n",
    "                    method_results['calinski_harabasz_scores'].append(calinski)\n",
    "                    method_results['davies_bouldin_scores'].append(davies_bouldin)\n",
    "                    \n",
    "                    if method == 'kmeans':\n",
    "                        method_results['inertias'].append(clusterer.inertia_)\n",
    "                    else:\n",
    "                        method_results['inertias'].append(0)  # Placeholder\n",
    "                else:\n",
    "                    # Invalid clustering\n",
    "                    method_results['silhouette_scores'].append(-1)\n",
    "                    method_results['calinski_harabasz_scores'].append(0)\n",
    "                    method_results['davies_bouldin_scores'].append(float('inf'))\n",
    "                    method_results['inertias'].append(float('inf'))\n",
    "            \n",
    "            results[method] = method_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def apply_clustering_algorithms(self, n_clusters=None):\n",
    "        \"\"\"Apply multiple clustering algorithms and compare results.\"\"\"\n",
    "        print(f\"üî¨ Applying multiple clustering algorithms...\")\n",
    "        \n",
    "        clustering_results = {}\n",
    "        \n",
    "        # Determine number of clusters if not specified\n",
    "        if n_clusters is None:\n",
    "            # Use heuristic: sqrt(n_samples/2)\n",
    "            n_clusters = max(3, min(15, int(np.sqrt(len(self.embeddings) / 2))))\n",
    "            print(f\"  Using heuristic n_clusters = {n_clusters}\")\n",
    "        \n",
    "        # 1. K-Means Clustering\n",
    "        try:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            kmeans_labels = kmeans.fit_predict(self.embeddings_scaled)\n",
    "            \n",
    "            clustering_results['kmeans'] = {\n",
    "                'labels': kmeans_labels,\n",
    "                'algorithm': 'K-Means',\n",
    "                'n_clusters': len(set(kmeans_labels)),\n",
    "                'silhouette_score': silhouette_score(self.embeddings_scaled, kmeans_labels),\n",
    "                'inertia': kmeans.inertia_,\n",
    "                'cluster_centers': kmeans.cluster_centers_\n",
    "            }\n",
    "            print(f\"  ‚úì K-Means: {len(set(kmeans_labels))} clusters, silhouette = {clustering_results['kmeans']['silhouette_score']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå K-Means failed: {e}\")\n",
    "        \n",
    "        # 2. Hierarchical Clustering\n",
    "        try:\n",
    "            hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "            hierarchical_labels = hierarchical.fit_predict(self.embeddings_scaled)\n",
    "            \n",
    "            clustering_results['hierarchical'] = {\n",
    "                'labels': hierarchical_labels,\n",
    "                'algorithm': 'Hierarchical (Ward)',\n",
    "                'n_clusters': len(set(hierarchical_labels)),\n",
    "                'silhouette_score': silhouette_score(self.embeddings_scaled, hierarchical_labels)\n",
    "            }\n",
    "            print(f\"  ‚úì Hierarchical: {len(set(hierarchical_labels))} clusters, silhouette = {clustering_results['hierarchical']['silhouette_score']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Hierarchical failed: {e}\")\n",
    "        \n",
    "        # 3. DBSCAN (density-based)\n",
    "        try:\n",
    "            # Find suitable eps using k-distance graph\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            k = 4\n",
    "            nbrs = NearestNeighbors(n_neighbors=k)\n",
    "            nbrs.fit(self.embeddings_scaled)\n",
    "            distances, indices = nbrs.kneighbors(self.embeddings_scaled)\n",
    "            distances = np.sort(distances[:, k-1], axis=0)\n",
    "            \n",
    "            # Use knee point as eps (simplified)\n",
    "            eps = np.percentile(distances, 90)\n",
    "            \n",
    "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "            dbscan_labels = dbscan.fit_predict(self.embeddings_scaled)\n",
    "            \n",
    "            n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "            n_noise = list(dbscan_labels).count(-1)\n",
    "            \n",
    "            if n_clusters_dbscan > 1:\n",
    "                # Calculate silhouette only for non-noise points\n",
    "                non_noise_mask = dbscan_labels != -1\n",
    "                if np.sum(non_noise_mask) > 1:\n",
    "                    silhouette = silhouette_score(self.embeddings_scaled[non_noise_mask], \n",
    "                                                dbscan_labels[non_noise_mask])\n",
    "                else:\n",
    "                    silhouette = -1\n",
    "            else:\n",
    "                silhouette = -1\n",
    "            \n",
    "            clustering_results['dbscan'] = {\n",
    "                'labels': dbscan_labels,\n",
    "                'algorithm': 'DBSCAN',\n",
    "                'n_clusters': n_clusters_dbscan,\n",
    "                'n_noise': n_noise,\n",
    "                'eps': eps,\n",
    "                'silhouette_score': silhouette\n",
    "            }\n",
    "            print(f\"  ‚úì DBSCAN: {n_clusters_dbscan} clusters, {n_noise} noise points, silhouette = {silhouette:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå DBSCAN failed: {e}\")\n",
    "        \n",
    "        # 4. Gaussian Mixture Model\n",
    "        try:\n",
    "            gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "            gmm_labels = gmm.fit_predict(self.embeddings_scaled)\n",
    "            \n",
    "            clustering_results['gmm'] = {\n",
    "                'labels': gmm_labels,\n",
    "                'algorithm': 'Gaussian Mixture',\n",
    "                'n_clusters': len(set(gmm_labels)),\n",
    "                'silhouette_score': silhouette_score(self.embeddings_scaled, gmm_labels),\n",
    "                'aic': gmm.aic(self.embeddings_scaled),\n",
    "                'bic': gmm.bic(self.embeddings_scaled)\n",
    "            }\n",
    "            print(f\"  ‚úì GMM: {len(set(gmm_labels))} clusters, silhouette = {clustering_results['gmm']['silhouette_score']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå GMM failed: {e}\")\n",
    "        \n",
    "        # 5. Spectral Clustering\n",
    "        try:\n",
    "            spectral = SpectralClustering(n_clusters=n_clusters, random_state=42, \n",
    "                                        affinity='rbf', gamma=1.0)\n",
    "            spectral_labels = spectral.fit_predict(self.embeddings_scaled)\n",
    "            \n",
    "            clustering_results['spectral'] = {\n",
    "                'labels': spectral_labels,\n",
    "                'algorithm': 'Spectral',\n",
    "                'n_clusters': len(set(spectral_labels)),\n",
    "                'silhouette_score': silhouette_score(self.embeddings_scaled, spectral_labels)\n",
    "            }\n",
    "            print(f\"  ‚úì Spectral: {len(set(spectral_labels))} clusters, silhouette = {clustering_results['spectral']['silhouette_score']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Spectral failed: {e}\")\n",
    "        \n",
    "        return clustering_results\n",
    "    \n",
    "    def analyze_cluster_characteristics(self, clustering_results):\n",
    "        \"\"\"Analyze the characteristics of discovered clusters.\"\"\"\n",
    "        print(f\"üîç Analyzing cluster characteristics...\")\n",
    "        \n",
    "        cluster_analyses = {}\n",
    "        \n",
    "        for method_name, result in clustering_results.items():\n",
    "            labels = result['labels']\n",
    "            unique_labels = set(labels)\n",
    "            \n",
    "            # Skip noise label for DBSCAN\n",
    "            if -1 in unique_labels:\n",
    "                unique_labels.remove(-1)\n",
    "            \n",
    "            cluster_info = []\n",
    "            \n",
    "            for cluster_id in unique_labels:\n",
    "                cluster_mask = labels == cluster_id\n",
    "                cluster_concepts = self.concept_df[cluster_mask]\n",
    "                cluster_embeddings = self.embeddings[cluster_mask]\n",
    "                \n",
    "                if len(cluster_concepts) > 0:\n",
    "                    # Analyze cluster composition\n",
    "                    concept_types = cluster_concepts['concept_type'].value_counts()\n",
    "                    complexity_dist = cluster_concepts['complexity'].value_counts()\n",
    "                    \n",
    "                    # Calculate cluster cohesion (intra-cluster similarity)\n",
    "                    if len(cluster_embeddings) > 1:\n",
    "                        pairwise_similarities = np.triu(np.dot(cluster_embeddings, cluster_embeddings.T), k=1)\n",
    "                        cohesion = np.mean(pairwise_similarities[pairwise_similarities > 0])\n",
    "                    else:\n",
    "                        cohesion = 1.0\n",
    "                    \n",
    "                    # Get representative concepts (closest to centroid)\n",
    "                    cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "                    distances_to_centroid = np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1)\n",
    "                    representative_indices = np.argsort(distances_to_centroid)[:3]\n",
    "                    representatives = cluster_concepts.iloc[representative_indices]['name'].tolist()\n",
    "                    \n",
    "                    # Determine cluster theme\n",
    "                    most_common_type = concept_types.index[0] if not concept_types.empty else 'unknown'\n",
    "                    theme = self._infer_cluster_theme(cluster_concepts['name'].tolist(), most_common_type)\n",
    "                    \n",
    "                    cluster_info.append({\n",
    "                        'cluster_id': cluster_id,\n",
    "                        'size': len(cluster_concepts),\n",
    "                        'cohesion': cohesion,\n",
    "                        'dominant_type': most_common_type,\n",
    "                        'type_distribution': dict(concept_types),\n",
    "                        'complexity_distribution': dict(complexity_dist),\n",
    "                        'representatives': representatives,\n",
    "                        'theme': theme,\n",
    "                        'diversity_score': len(concept_types) / len(cluster_concepts)  # Type diversity\n",
    "                    })\n",
    "            \n",
    "            cluster_analyses[method_name] = {\n",
    "                'algorithm': result['algorithm'],\n",
    "                'n_clusters': len(cluster_info),\n",
    "                'silhouette_score': result.get('silhouette_score', 0),\n",
    "                'clusters': cluster_info,\n",
    "                'average_cluster_size': np.mean([c['size'] for c in cluster_info]) if cluster_info else 0,\n",
    "                'average_cohesion': np.mean([c['cohesion'] for c in cluster_info]) if cluster_info else 0\n",
    "            }\n",
    "        \n",
    "        return cluster_analyses\n",
    "    \n",
    "    def _infer_cluster_theme(self, concept_names, dominant_type):\n",
    "        \"\"\"Infer the main theme of a cluster based on concept names.\"\"\"\n",
    "        # Count common words across concept names\n",
    "        all_words = []\n",
    "        for name in concept_names:\n",
    "            words = name.lower().split()\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Remove common stop words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "        filtered_words = {word: count for word, count in word_counts.items() \n",
    "                         if word not in stop_words and len(word) > 2}\n",
    "        \n",
    "        if filtered_words:\n",
    "            # Get most common meaningful words\n",
    "            top_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            theme_words = [word for word, count in top_words if count > 1]\n",
    "            \n",
    "            if theme_words:\n",
    "                return f\"{dominant_type}: {', '.join(theme_words)}\"\n",
    "        \n",
    "        return f\"{dominant_type}: general\"\n",
    "\n",
    "# Initialize clusterer and run analysis\n",
    "if cluster_data:\n",
    "    clusterer = MultiAlgorithmClusterer(cluster_data)\n",
    "    \n",
    "    print(\"\\nüéØ MULTI-ALGORITHM CLUSTERING ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Apply clustering algorithms\n",
    "    clustering_results = clusterer.apply_clustering_algorithms(n_clusters=8)\n",
    "    \n",
    "    # Analyze cluster characteristics\n",
    "    cluster_analyses = clusterer.analyze_cluster_characteristics(clustering_results)\n",
    "    \n",
    "    print(f\"\\nüìä CLUSTERING COMPARISON SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for method_name, analysis in cluster_analyses.items():\n",
    "        print(f\"{analysis['algorithm']:20}: {analysis['n_clusters']:2d} clusters, \"\n",
    "              f\"silhouette = {analysis['silhouette_score']:5.3f}, \"\n",
    "              f\"avg_size = {analysis['average_cluster_size']:4.1f}\")\n",
    "    \n",
    "    # Show best clustering result details\n",
    "    best_method = max(cluster_analyses.items(), \n",
    "                     key=lambda x: x[1]['silhouette_score'] if x[1]['silhouette_score'] > -1 else -2)\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Method: {best_method[0]} ({best_method[1]['algorithm']})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, cluster in enumerate(best_method[1]['clusters'][:6]):  # Show top 6 clusters\n",
    "        print(f\"Cluster {cluster['cluster_id']:2d}: {cluster['size']:3d} concepts | {cluster['theme'][:40]}\")\n",
    "        print(f\"           Examples: {', '.join(cluster['representatives'][:2])}...\")\n",
    "        print(f\"           Cohesion: {cluster['cohesion']:.3f}, Diversity: {cluster['diversity_score']:.3f}\")\n",
    "        print()\n",
    "        \n",
    "else:\n",
    "    print(\"Clusterer not available - need clustering data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hierarchical Taxonomy Construction\n",
    "\n",
    "Build hierarchical taxonomies from the discovered clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomyBuilder:\n",
    "    def __init__(self, cluster_data, clustering_results):\n",
    "        \"\"\"Initialize taxonomy builder with clustering results.\"\"\"\n",
    "        self.cluster_data = cluster_data\n",
    "        self.clustering_results = clustering_results\n",
    "        self.embeddings = cluster_data['embeddings']\n",
    "        self.concept_df = cluster_data['concept_df']\n",
    "        \n",
    "        print(f\"üå≥ Taxonomy builder initialized\")\n",
    "    \n",
    "    def build_hierarchical_taxonomy(self, method='ward', max_depth=4):\n",
    "        \"\"\"Build hierarchical taxonomy using agglomerative clustering.\"\"\"\n",
    "        print(f\"üèóÔ∏è Building hierarchical taxonomy...\")\n",
    "        \n",
    "        # Standardize embeddings\n",
    "        scaler = StandardScaler()\n",
    "        embeddings_scaled = scaler.fit_transform(self.embeddings)\n",
    "        \n",
    "        # Compute linkage matrix\n",
    "        print(f\"  Computing linkage matrix with {method} method...\")\n",
    "        linkage_matrix = linkage(embeddings_scaled, method=method)\n",
    "        \n",
    "        # Build taxonomy at different levels\n",
    "        taxonomy_levels = {}\n",
    "        \n",
    "        for depth in range(2, max_depth + 1):\n",
    "            n_clusters = min(depth * 2, len(self.concept_df) // 10)  # Adaptive cluster count\n",
    "            n_clusters = max(2, n_clusters)\n",
    "            \n",
    "            labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "            \n",
    "            # Analyze clusters at this level\n",
    "            level_clusters = self._analyze_taxonomy_level(labels, depth)\n",
    "            taxonomy_levels[depth] = {\n",
    "                'n_clusters': n_clusters,\n",
    "                'labels': labels,\n",
    "                'clusters': level_clusters,\n",
    "                'silhouette_score': silhouette_score(embeddings_scaled, labels) if len(set(labels)) > 1 else -1\n",
    "            }\n",
    "            \n",
    "            print(f\"  Level {depth}: {n_clusters} clusters, silhouette = {taxonomy_levels[depth]['silhouette_score']:.3f}\")\n",
    "        \n",
    "        # Build tree structure\n",
    "        taxonomy_tree = self._build_tree_structure(linkage_matrix, taxonomy_levels)\n",
    "        \n",
    "        return {\n",
    "            'linkage_matrix': linkage_matrix,\n",
    "            'taxonomy_levels': taxonomy_levels,\n",
    "            'taxonomy_tree': taxonomy_tree,\n",
    "            'method': method\n",
    "        }\n",
    "    \n",
    "    def _analyze_taxonomy_level(self, labels, depth):\n",
    "        \"\"\"Analyze clusters at a specific taxonomy level.\"\"\"\n",
    "        unique_labels = set(labels)\n",
    "        level_clusters = {}\n",
    "        \n",
    "        for cluster_id in unique_labels:\n",
    "            cluster_mask = labels == cluster_id\n",
    "            cluster_concepts = self.concept_df[cluster_mask]\n",
    "            cluster_embeddings = self.embeddings[cluster_mask]\n",
    "            \n",
    "            if len(cluster_concepts) > 0:\n",
    "                # Calculate cluster statistics\n",
    "                concept_types = cluster_concepts['concept_type'].value_counts()\n",
    "                \n",
    "                # Find representative concepts\n",
    "                cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "                distances = np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1)\n",
    "                representative_idx = np.argmin(distances)\n",
    "                representative_concept = cluster_concepts.iloc[representative_idx]['name']\n",
    "                \n",
    "                # Generate taxonomy label\n",
    "                dominant_type = concept_types.index[0] if not concept_types.empty else 'general'\n",
    "                taxonomy_label = self._generate_taxonomy_label(cluster_concepts, dominant_type, depth)\n",
    "                \n",
    "                level_clusters[cluster_id] = {\n",
    "                    'taxonomy_label': taxonomy_label,\n",
    "                    'size': len(cluster_concepts),\n",
    "                    'dominant_type': dominant_type,\n",
    "                    'type_distribution': dict(concept_types),\n",
    "                    'representative_concept': representative_concept,\n",
    "                    'concept_list': cluster_concepts['name'].tolist(),\n",
    "                    'diversity': len(concept_types) / len(cluster_concepts)\n",
    "                }\n",
    "        \n",
    "        return level_clusters\n",
    "    \n",
    "    def _generate_taxonomy_label(self, cluster_concepts, dominant_type, depth):\n",
    "        \"\"\"Generate hierarchical taxonomy labels.\"\"\"\n",
    "        \n",
    "        # Analyze concept names to find common themes\n",
    "        concept_names = cluster_concepts['name'].tolist()\n",
    "        \n",
    "        # Extract common words (excluding stop words)\n",
    "        all_words = []\n",
    "        for name in concept_names:\n",
    "            words = [word.lower() for word in name.split() if len(word) > 2]\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        word_counts = Counter(all_words)\n",
    "        common_words = [word for word, count in word_counts.most_common(5) if count > 1]\n",
    "        \n",
    "        # Create hierarchical label based on depth\n",
    "        if depth == 2:  # Top level - broad categories\n",
    "            broad_categories = {\n",
    "                'biological_entity': 'Biological Entities',\n",
    "                'methodology': 'Methods & Algorithms', \n",
    "                'data_resource': 'Data Resources',\n",
    "                'software_tool': 'Software Tools',\n",
    "                'database': 'Databases & Repositories',\n",
    "                'medical_condition': 'Medical Conditions',\n",
    "                'general': 'General Concepts'\n",
    "            }\n",
    "            return broad_categories.get(dominant_type, 'General Concepts')\n",
    "        \n",
    "        elif depth == 3:  # Mid level - more specific\n",
    "            if common_words:\n",
    "                theme_word = common_words[0].title()\n",
    "                return f\"{dominant_type.replace('_', ' ').title()}: {theme_word}-related\"\n",
    "            else:\n",
    "                return f\"{dominant_type.replace('_', ' ').title()}: General\"\n",
    "        \n",
    "        else:  # Deeper levels - very specific\n",
    "            if len(common_words) >= 2:\n",
    "                theme = ' & '.join(common_words[:2]).title()\n",
    "                return f\"{theme} Concepts\"\n",
    "            elif common_words:\n",
    "                return f\"{common_words[0].title()} Specialized\"\n",
    "            else:\n",
    "                return f\"Cluster {depth}-{len(cluster_concepts)}\"\n",
    "    \n",
    "    def _build_tree_structure(self, linkage_matrix, taxonomy_levels):\n",
    "        \"\"\"Build tree structure from linkage matrix.\"\"\"\n",
    "        print(f\"  Building tree structure...\")\n",
    "        \n",
    "        # Convert linkage matrix to tree\n",
    "        tree = to_tree(linkage_matrix)\n",
    "        \n",
    "        # Extract tree structure with taxonomy labels\n",
    "        def traverse_tree(node, level=0, max_level=3):\n",
    "            if level > max_level:\n",
    "                return None\n",
    "            \n",
    "            tree_node = {\n",
    "                'id': node.id,\n",
    "                'level': level,\n",
    "                'distance': node.dist,\n",
    "                'count': node.count\n",
    "            }\n",
    "            \n",
    "            if node.is_leaf():\n",
    "                # Leaf node - individual concept\n",
    "                concept = self.concept_df.iloc[node.id]\n",
    "                tree_node.update({\n",
    "                    'type': 'leaf',\n",
    "                    'concept_name': concept['name'],\n",
    "                    'concept_type': concept['concept_type']\n",
    "                })\n",
    "            else:\n",
    "                # Internal node - cluster\n",
    "                tree_node.update({\n",
    "                    'type': 'internal',\n",
    "                    'left': traverse_tree(node.left, level + 1, max_level),\n",
    "                    'right': traverse_tree(node.right, level + 1, max_level)\n",
    "                })\n",
    "                \n",
    "                # Add taxonomy information if available\n",
    "                if level + 2 in taxonomy_levels:  # Map to appropriate taxonomy level\n",
    "                    # This is a simplified mapping - in practice, you'd need more sophisticated tree-to-cluster mapping\n",
    "                    tree_node['taxonomy_info'] = f\"Internal Node L{level}\"\n",
    "            \n",
    "            return tree_node\n",
    "        \n",
    "        return traverse_tree(tree)\n",
    "    \n",
    "    def create_concept_hierarchy_graph(self, taxonomy_result, max_concepts=50):\n",
    "        \"\"\"Create a graph representation of the concept hierarchy.\"\"\"\n",
    "        print(f\"üìä Creating concept hierarchy graph...\")\n",
    "        \n",
    "        # Use the best taxonomy level (highest silhouette score)\n",
    "        best_level = max(taxonomy_result['taxonomy_levels'].items(), \n",
    "                        key=lambda x: x[1]['silhouette_score'])\n",
    "        \n",
    "        level_data = best_level[1]\n",
    "        clusters = level_data['clusters']\n",
    "        \n",
    "        # Build NetworkX graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add cluster nodes (taxonomy categories)\n",
    "        cluster_nodes = {}\n",
    "        for cluster_id, cluster_info in clusters.items():\n",
    "            cluster_node = f\"cluster_{cluster_id}\"\n",
    "            G.add_node(cluster_node,\n",
    "                      node_type='cluster',\n",
    "                      label=cluster_info['taxonomy_label'],\n",
    "                      size=cluster_info['size'],\n",
    "                      dominant_type=cluster_info['dominant_type'])\n",
    "            cluster_nodes[cluster_id] = cluster_node\n",
    "        \n",
    "        # Add concept nodes (sample from each cluster)\n",
    "        concept_count = 0\n",
    "        for cluster_id, cluster_info in clusters.items():\n",
    "            if concept_count >= max_concepts:\n",
    "                break\n",
    "                \n",
    "            cluster_node = cluster_nodes[cluster_id]\n",
    "            \n",
    "            # Add representative concepts from this cluster\n",
    "            concepts_to_add = min(5, cluster_info['size'], max_concepts - concept_count)\n",
    "            \n",
    "            for i in range(concepts_to_add):\n",
    "                concept_name = cluster_info['concept_list'][i]\n",
    "                concept_node = f\"concept_{concept_count}\"\n",
    "                \n",
    "                G.add_node(concept_node,\n",
    "                          node_type='concept', \n",
    "                          label=concept_name[:30],  # Truncate long names\n",
    "                          full_name=concept_name,\n",
    "                          cluster_id=cluster_id)\n",
    "                \n",
    "                # Connect concept to its cluster\n",
    "                G.add_edge(cluster_node, concept_node, edge_type='contains')\n",
    "                \n",
    "                concept_count += 1\n",
    "        \n",
    "        print(f\"  Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "        \n",
    "        return G\n",
    "\n",
    "# Build taxonomies\n",
    "if 'clustering_results' in locals() and cluster_data:\n",
    "    taxonomy_builder = TaxonomyBuilder(cluster_data, clustering_results)\n",
    "    \n",
    "    print(\"\\nüå≥ HIERARCHICAL TAXONOMY CONSTRUCTION\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Build hierarchical taxonomy\n",
    "    taxonomy_result = taxonomy_builder.build_hierarchical_taxonomy(\n",
    "        method='ward', max_depth=4\n",
    "    )\n",
    "    \n",
    "    # Show taxonomy levels\n",
    "    print(f\"\\nüìä TAXONOMY LEVEL ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for depth, level_data in taxonomy_result['taxonomy_levels'].items():\n",
    "        print(f\"Level {depth}: {level_data['n_clusters']} categories, silhouette = {level_data['silhouette_score']:.3f}\")\n",
    "        \n",
    "        # Show categories at this level\n",
    "        for cluster_id, cluster_info in sorted(level_data['clusters'].items(), \n",
    "                                             key=lambda x: x[1]['size'], reverse=True)[:4]:\n",
    "            print(f\"  ‚Ä¢ {cluster_info['taxonomy_label']:30} ({cluster_info['size']:3d} concepts) - {cluster_info['dominant_type']}\")\n",
    "        print()\n",
    "    \n",
    "    # Create concept hierarchy graph\n",
    "    hierarchy_graph = taxonomy_builder.create_concept_hierarchy_graph(\n",
    "        taxonomy_result, max_concepts=40\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Taxonomy construction completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Taxonomy builder not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Interactive Taxonomy Visualization\n",
    "\n",
    "Create interactive visualizations to explore the discovered taxonomies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_taxonomy_visualizations(cluster_data, clustering_results, taxonomy_result):\n",
    "    \"\"\"Create comprehensive taxonomy visualizations.\"\"\"\n",
    "    \n",
    "    print(f\"üé® Creating taxonomy visualizations...\")\n",
    "    \n",
    "    # 1. Dimensionality reduction for visualization\n",
    "    print(f\"  Reducing dimensionality for visualization...\")\n",
    "    \n",
    "    embeddings = cluster_data['embeddings']\n",
    "    concept_df = cluster_data['concept_df']\n",
    "    \n",
    "    # UMAP reduction\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=42, min_dist=0.1, n_neighbors=15)\n",
    "    embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Get best clustering result\n",
    "    best_clustering = max(clustering_results.items(), \n",
    "                         key=lambda x: x[1].get('silhouette_score', -2))\n",
    "    best_method = best_clustering[0]\n",
    "    best_labels = best_clustering[1]['labels']\n",
    "    \n",
    "    print(f\"  Using {best_method} clustering for visualization\")\n",
    "    \n",
    "    # 2. Create main clustering visualization\n",
    "    fig_main = go.Figure()\n",
    "    \n",
    "    # Color palette for clusters\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    unique_labels = sorted(set(best_labels))\n",
    "    if -1 in unique_labels:  # Handle noise points for DBSCAN\n",
    "        unique_labels.remove(-1)\n",
    "        unique_labels.append(-1)  # Put noise at end\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        cluster_mask = best_labels == label\n",
    "        cluster_concepts = concept_df[cluster_mask]\n",
    "        cluster_2d = embeddings_2d[cluster_mask]\n",
    "        \n",
    "        if len(cluster_2d) > 0:\n",
    "            color = 'lightgray' if label == -1 else colors[i % len(colors)]\n",
    "            name = 'Noise' if label == -1 else f'Cluster {label}'\n",
    "            \n",
    "            # Determine cluster theme\n",
    "            if len(cluster_concepts) > 0:\n",
    "                most_common_type = cluster_concepts['concept_type'].mode()\n",
    "                if not most_common_type.empty:\n",
    "                    theme = most_common_type.iloc[0].replace('_', ' ').title()\n",
    "                    name = f'{name}: {theme}'\n",
    "            \n",
    "            fig_main.add_trace(go.Scatter(\n",
    "                x=cluster_2d[:, 0],\n",
    "                y=cluster_2d[:, 1],\n",
    "                mode='markers',\n",
    "                name=name,\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=color,\n",
    "                    opacity=0.7,\n",
    "                    line=dict(width=1, color='white')\n",
    "                ),\n",
    "                text=cluster_concepts['name'].tolist(),\n",
    "                hovertemplate='<b>%{text}</b><br>' +\n",
    "                             f'Cluster: {name}<br>' +\n",
    "                             'Type: %{customdata}<br>' +\n",
    "                             '<extra></extra>',\n",
    "                customdata=cluster_concepts['concept_type'].tolist()\n",
    "            ))\n",
    "    \n",
    "    fig_main.update_layout(\n",
    "        title=f'Concept Clustering Visualization ({best_method.title()})<br><sub>UMAP projection of {len(embeddings)} concepts</sub>',\n",
    "        xaxis_title='UMAP Dimension 1',\n",
    "        yaxis_title='UMAP Dimension 2',\n",
    "        width=1000,\n",
    "        height=700,\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    # 3. Create dendrogram for hierarchical taxonomy\n",
    "    if 'linkage_matrix' in taxonomy_result:\n",
    "        print(f\"  Creating dendrogram...\")\n",
    "        \n",
    "        fig_dendro, ax_dendro = plt.subplots(figsize=(15, 8))\n",
    "        \n",
    "        # Sample concepts for cleaner dendrogram\n",
    "        max_leaves = 50\n",
    "        if len(concept_df) > max_leaves:\n",
    "            sample_indices = np.random.choice(len(concept_df), max_leaves, replace=False)\n",
    "            sample_linkage = linkage(embeddings[sample_indices], method='ward')\n",
    "            sample_labels = [concept_df.iloc[i]['name'][:20] for i in sample_indices]\n",
    "        else:\n",
    "            sample_linkage = taxonomy_result['linkage_matrix']\n",
    "            sample_labels = [name[:20] for name in concept_df['name'].tolist()]\n",
    "        \n",
    "        dendrogram(sample_linkage, \n",
    "                  labels=sample_labels,\n",
    "                  leaf_rotation=90,\n",
    "                  leaf_font_size=8,\n",
    "                  ax=ax_dendro)\n",
    "        \n",
    "        ax_dendro.set_title('Hierarchical Concept Taxonomy (Sample)', fontsize=14)\n",
    "        ax_dendro.set_xlabel('Concepts', fontsize=12)\n",
    "        ax_dendro.set_ylabel('Distance', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. Create taxonomy level comparison\n",
    "    if 'taxonomy_levels' in taxonomy_result:\n",
    "        print(f\"  Creating taxonomy level comparison...\")\n",
    "        \n",
    "        fig_levels = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['Cluster Sizes by Level', 'Silhouette Scores by Level', \n",
    "                          'Category Distribution', 'Concept Type Analysis'],\n",
    "            specs=[[{'type': 'bar'}, {'type': 'scatter'}],\n",
    "                   [{'type': 'bar'}, {'type': 'pie'}]]\n",
    "        )\n",
    "        \n",
    "        # Analyze taxonomy levels\n",
    "        levels = list(taxonomy_result['taxonomy_levels'].keys())\n",
    "        silhouette_scores = [taxonomy_result['taxonomy_levels'][level]['silhouette_score'] for level in levels]\n",
    "        cluster_counts = [taxonomy_result['taxonomy_levels'][level]['n_clusters'] for level in levels]\n",
    "        \n",
    "        # Plot 1: Cluster counts by level\n",
    "        fig_levels.add_trace(\n",
    "            go.Bar(x=levels, y=cluster_counts, name='Clusters', marker_color='skyblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 2: Silhouette scores\n",
    "        fig_levels.add_trace(\n",
    "            go.Scatter(x=levels, y=silhouette_scores, mode='lines+markers', \n",
    "                      name='Silhouette', marker_color='orange'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Plot 3: Category sizes for best level\n",
    "        best_tax_level = max(taxonomy_result['taxonomy_levels'].items(), \n",
    "                           key=lambda x: x[1]['silhouette_score'])\n",
    "        \n",
    "        category_sizes = [info['size'] for info in best_tax_level[1]['clusters'].values()]\n",
    "        category_labels = [info['taxonomy_label'][:20] for info in best_tax_level[1]['clusters'].values()]\n",
    "        \n",
    "        fig_levels.add_trace(\n",
    "            go.Bar(x=category_labels, y=category_sizes, name='Category Sizes', \n",
    "                  marker_color='lightgreen'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 4: Concept type distribution\n",
    "        type_counts = concept_df['concept_type'].value_counts()\n",
    "        fig_levels.add_trace(\n",
    "            go.Pie(labels=type_counts.index, values=type_counts.values, name=\"Types\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig_levels.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"Taxonomy Analysis Dashboard\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig_levels.update_xaxes(title_text=\"Taxonomy Level\", row=1, col=1)\n",
    "        fig_levels.update_xaxes(title_text=\"Taxonomy Level\", row=1, col=2)\n",
    "        fig_levels.update_xaxes(title_text=\"Categories\", tickangle=45, row=2, col=1)\n",
    "        \n",
    "        fig_levels.update_yaxes(title_text=\"Number of Clusters\", row=1, col=1)\n",
    "        fig_levels.update_yaxes(title_text=\"Silhouette Score\", row=1, col=2)\n",
    "        fig_levels.update_yaxes(title_text=\"Number of Concepts\", row=2, col=1)\n",
    "        \n",
    "        fig_levels.show()\n",
    "    \n",
    "    return fig_main\n",
    "\n",
    "def create_interactive_taxonomy_explorer(hierarchy_graph):\n",
    "    \"\"\"Create interactive taxonomy explorer.\"\"\"\n",
    "    \n",
    "    print(f\"üîç Creating interactive taxonomy explorer...\")\n",
    "    \n",
    "    # Layout the hierarchy graph\n",
    "    pos = nx.spring_layout(hierarchy_graph, k=3, iterations=50)\n",
    "    \n",
    "    # Separate cluster and concept nodes\n",
    "    cluster_nodes = [(node, data) for node, data in hierarchy_graph.nodes(data=True) \n",
    "                    if data['node_type'] == 'cluster']\n",
    "    concept_nodes = [(node, data) for node, data in hierarchy_graph.nodes(data=True) \n",
    "                    if data['node_type'] == 'concept']\n",
    "    \n",
    "    # Create edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    \n",
    "    for edge in hierarchy_graph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "    \n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines',\n",
    "        name='Connections'\n",
    "    )\n",
    "    \n",
    "    # Create cluster node trace\n",
    "    cluster_x = [pos[node][0] for node, data in cluster_nodes]\n",
    "    cluster_y = [pos[node][1] for node, data in cluster_nodes]\n",
    "    cluster_text = [data['label'] for node, data in cluster_nodes]\n",
    "    cluster_sizes = [min(50, max(20, data['size'] * 2)) for node, data in cluster_nodes]\n",
    "    cluster_types = [data['dominant_type'] for node, data in cluster_nodes]\n",
    "    \n",
    "    cluster_trace = go.Scatter(\n",
    "        x=cluster_x, y=cluster_y,\n",
    "        mode='markers+text',\n",
    "        text=cluster_text,\n",
    "        textposition=\"middle center\",\n",
    "        name='Categories',\n",
    "        marker=dict(\n",
    "            size=cluster_sizes,\n",
    "            color='lightblue',\n",
    "            line=dict(width=2, color='blue')\n",
    "        ),\n",
    "        customdata=cluster_types,\n",
    "        hovertemplate='<b>%{text}</b><br>' +\n",
    "                     'Type: %{customdata}<br>' +\n",
    "                     'Size: %{marker.size}<br>' +\n",
    "                     '<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    # Create concept node trace\n",
    "    concept_x = [pos[node][0] for node, data in concept_nodes]\n",
    "    concept_y = [pos[node][1] for node, data in concept_nodes]\n",
    "    concept_text = [data['label'] for node, data in concept_nodes]\n",
    "    concept_full_names = [data['full_name'] for node, data in concept_nodes]\n",
    "    \n",
    "    concept_trace = go.Scatter(\n",
    "        x=concept_x, y=concept_y,\n",
    "        mode='markers',\n",
    "        name='Concepts',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color='lightcoral',\n",
    "            line=dict(width=1, color='red')\n",
    "        ),\n",
    "        customdata=concept_full_names,\n",
    "        hovertemplate='<b>%{customdata}</b><br>' +\n",
    "                     'Category: Connected above<br>' +\n",
    "                     '<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, cluster_trace, concept_trace],\n",
    "                   layout=go.Layout(\n",
    "                        title='Interactive Taxonomy Explorer<br><sub>Blue: Categories, Red: Concepts, Lines: Hierarchical relationships</sub>',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=True,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=60),\n",
    "                        annotations=[ dict(\n",
    "                            text=\"Click and drag to explore the taxonomy. Hover for details.\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002 ) ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        width=1200,\n",
    "                        height=800\n",
    "                   ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create visualizations\n",
    "if all(var in locals() for var in ['cluster_data', 'clustering_results', 'taxonomy_result']):\n",
    "    print(\"\\nüé® TAXONOMY VISUALIZATION DASHBOARD\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create main visualizations\n",
    "    main_fig = create_taxonomy_visualizations(cluster_data, clustering_results, taxonomy_result)\n",
    "    main_fig.show()\n",
    "    \n",
    "    # Create interactive explorer\n",
    "    if 'hierarchy_graph' in locals():\n",
    "        explorer_fig = create_interactive_taxonomy_explorer(hierarchy_graph)\n",
    "        explorer_fig.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualization dashboard created!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create visualizations - missing required data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Taxonomy Quality Assessment\n",
    "\n",
    "Evaluate the quality of discovered taxonomies against various criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomyQualityAssessor:\n",
    "    def __init__(self, cluster_data, clustering_results, taxonomy_result):\n",
    "        \"\"\"Initialize taxonomy quality assessor.\"\"\"\n",
    "        self.cluster_data = cluster_data\n",
    "        self.clustering_results = clustering_results\n",
    "        self.taxonomy_result = taxonomy_result\n",
    "        self.embeddings = cluster_data['embeddings']\n",
    "        self.concept_df = cluster_data['concept_df']\n",
    "        \n",
    "        print(f\"üìè Taxonomy quality assessor initialized\")\n",
    "    \n",
    "    def assess_clustering_quality(self):\n",
    "        \"\"\"Assess the quality of different clustering approaches.\"\"\"\n",
    "        print(f\"üìä Assessing clustering quality...\")\n",
    "        \n",
    "        quality_metrics = {}\n",
    "        \n",
    "        # Standardize embeddings for consistent evaluation\n",
    "        scaler = StandardScaler()\n",
    "        embeddings_scaled = scaler.fit_transform(self.embeddings)\n",
    "        \n",
    "        for method_name, result in self.clustering_results.items():\n",
    "            labels = result['labels']\n",
    "            \n",
    "            # Skip if clustering failed or has only one cluster\n",
    "            unique_labels = set(labels)\n",
    "            if len(unique_labels) <= 1:\n",
    "                continue\n",
    "            \n",
    "            # Remove noise points for DBSCAN evaluation\n",
    "            if -1 in unique_labels:\n",
    "                non_noise_mask = labels != -1\n",
    "                if np.sum(non_noise_mask) < len(labels) * 0.1:  # Too many noise points\n",
    "                    continue\n",
    "                eval_embeddings = embeddings_scaled[non_noise_mask]\n",
    "                eval_labels = labels[non_noise_mask]\n",
    "            else:\n",
    "                eval_embeddings = embeddings_scaled\n",
    "                eval_labels = labels\n",
    "            \n",
    "            if len(set(eval_labels)) <= 1:\n",
    "                continue\n",
    "            \n",
    "            # Calculate quality metrics\n",
    "            try:\n",
    "                silhouette = silhouette_score(eval_embeddings, eval_labels)\n",
    "                calinski_harabasz = calinski_harabasz_score(eval_embeddings, eval_labels)\n",
    "                davies_bouldin = davies_bouldin_score(eval_embeddings, eval_labels)\n",
    "                \n",
    "                # Calculate additional metrics\n",
    "                n_clusters = len(set(eval_labels))\n",
    "                cluster_sizes = [np.sum(eval_labels == label) for label in set(eval_labels)]\n",
    "                size_std = np.std(cluster_sizes)\n",
    "                size_balance = 1 - (size_std / np.mean(cluster_sizes))  # Closer to 1 = more balanced\n",
    "                \n",
    "                # Semantic coherence (based on concept types)\n",
    "                semantic_coherence = self._calculate_semantic_coherence(eval_labels)\n",
    "                \n",
    "                quality_metrics[method_name] = {\n",
    "                    'algorithm': result['algorithm'],\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'silhouette_score': silhouette,\n",
    "                    'calinski_harabasz_score': calinski_harabasz,\n",
    "                    'davies_bouldin_score': davies_bouldin,  # Lower is better\n",
    "                    'size_balance': max(0, size_balance),\n",
    "                    'semantic_coherence': semantic_coherence,\n",
    "                    'cluster_sizes': cluster_sizes,\n",
    "                    'n_noise_points': np.sum(labels == -1) if -1 in set(labels) else 0\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Failed to evaluate {method_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return quality_metrics\n",
    "    \n",
    "    def _calculate_semantic_coherence(self, labels):\n",
    "        \"\"\"Calculate semantic coherence based on concept types within clusters.\"\"\"\n",
    "        coherence_scores = []\n",
    "        \n",
    "        for cluster_id in set(labels):\n",
    "            cluster_mask = labels == cluster_id\n",
    "            cluster_concepts = self.concept_df[cluster_mask]\n",
    "            \n",
    "            if len(cluster_concepts) <= 1:\n",
    "                coherence_scores.append(1.0)  # Single concept is perfectly coherent\n",
    "                continue\n",
    "            \n",
    "            # Calculate type diversity within cluster (lower diversity = higher coherence)\n",
    "            type_counts = cluster_concepts['concept_type'].value_counts()\n",
    "            dominant_type_ratio = type_counts.iloc[0] / len(cluster_concepts)\n",
    "            \n",
    "            # Coherence = how dominated the cluster is by its main type\n",
    "            coherence_scores.append(dominant_type_ratio)\n",
    "        \n",
    "        return np.mean(coherence_scores)\n",
    "    \n",
    "    def assess_taxonomy_hierarchy(self):\n",
    "        \"\"\"Assess the quality of hierarchical taxonomy structure.\"\"\"\n",
    "        print(f\"üå≥ Assessing taxonomy hierarchy quality...\")\n",
    "        \n",
    "        hierarchy_quality = {}\n",
    "        \n",
    "        if 'taxonomy_levels' not in self.taxonomy_result:\n",
    "            return {}\n",
    "        \n",
    "        taxonomy_levels = self.taxonomy_result['taxonomy_levels']\n",
    "        \n",
    "        # Analyze each taxonomy level\n",
    "        level_qualities = []\n",
    "        \n",
    "        for level, level_data in taxonomy_levels.items():\n",
    "            clusters = level_data['clusters']\n",
    "            \n",
    "            # Calculate level-specific metrics\n",
    "            cluster_sizes = [info['size'] for info in clusters.values()]\n",
    "            cluster_diversities = [info['diversity'] for info in clusters.values()]\n",
    "            \n",
    "            level_quality = {\n",
    "                'level': level,\n",
    "                'n_clusters': level_data['n_clusters'],\n",
    "                'silhouette_score': level_data['silhouette_score'],\n",
    "                'average_cluster_size': np.mean(cluster_sizes),\n",
    "                'size_standard_deviation': np.std(cluster_sizes),\n",
    "                'average_diversity': np.mean(cluster_diversities),\n",
    "                'balance_score': 1 - (np.std(cluster_sizes) / np.mean(cluster_sizes)) if cluster_sizes else 0\n",
    "            }\n",
    "            \n",
    "            # Calculate taxonomy label quality\n",
    "            label_lengths = [len(info['taxonomy_label']) for info in clusters.values()]\n",
    "            level_quality['label_consistency'] = 1 - (np.std(label_lengths) / np.mean(label_lengths)) if label_lengths else 0\n",
    "            \n",
    "            level_qualities.append(level_quality)\n",
    "        \n",
    "        # Overall hierarchy assessment\n",
    "        hierarchy_quality = {\n",
    "            'n_levels': len(taxonomy_levels),\n",
    "            'level_qualities': level_qualities,\n",
    "            'best_level': max(level_qualities, key=lambda x: x['silhouette_score']),\n",
    "            'hierarchy_depth_score': min(1.0, len(taxonomy_levels) / 4.0),  # Ideal depth around 3-4\n",
    "            'overall_coherence': np.mean([lq['silhouette_score'] for lq in level_qualities])\n",
    "        }\n",
    "        \n",
    "        return hierarchy_quality\n",
    "    \n",
    "    def validate_against_original_ontology(self):\n",
    "        \"\"\"Validate discovered clusters against original ontology structure.\"\"\"\n",
    "        print(f\"‚úÖ Validating against original ontology...\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        if not self.cluster_data['ontology_structure']:\n",
    "            print(f\"  ‚ö†Ô∏è No original ontology structure available\")\n",
    "            return {'error': 'No original ontology structure available'}\n",
    "        \n",
    "        # Get original ontology structure\n",
    "        edge_index = self.cluster_data['ontology_structure']['edge_index']\n",
    "        class_mapping = self.cluster_data['ontology_structure']['class_mapping']\n",
    "        \n",
    "        # Map ontology classes to our concept indices\n",
    "        ontology_to_concept = {}\n",
    "        for ont_class, ont_idx in class_mapping.items():\n",
    "            class_iri = ont_class.iri if hasattr(ont_class, 'iri') else str(ont_class)\n",
    "            \n",
    "            # Find matching concept\n",
    "            matching_concepts = self.concept_df[self.concept_df['node_id'] == class_iri]\n",
    "            if not matching_concepts.empty:\n",
    "                concept_idx = matching_concepts.index[0]\n",
    "                ontology_to_concept[ont_idx] = concept_idx\n",
    "        \n",
    "        if len(ontology_to_concept) < 10:\n",
    "            return {'error': 'Insufficient overlap between ontology and concepts'}\n",
    "        \n",
    "        print(f\"  Found {len(ontology_to_concept)} matching concepts\")\n",
    "        \n",
    "        # Analyze clustering consistency with ontology hierarchy\n",
    "        for method_name, result in self.clustering_results.items():\n",
    "            labels = result['labels']\n",
    "            \n",
    "            # Calculate consistency score\n",
    "            consistency_score = self._calculate_hierarchy_consistency(\n",
    "                labels, edge_index, ontology_to_concept\n",
    "            )\n",
    "            \n",
    "            validation_results[method_name] = {\n",
    "                'hierarchy_consistency': consistency_score,\n",
    "                'coverage': len(ontology_to_concept) / len(class_mapping)\n",
    "            }\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def _calculate_hierarchy_consistency(self, labels, edge_index, ontology_to_concept):\n",
    "        \"\"\"Calculate how well clusters respect original ontology hierarchy.\"\"\"\n",
    "        \n",
    "        consistent_pairs = 0\n",
    "        total_pairs = 0\n",
    "        \n",
    "        # Check hierarchical relationships\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            parent_ont_idx = int(edge_index[1, i])  # Parent in hierarchy\n",
    "            child_ont_idx = int(edge_index[0, i])   # Child in hierarchy\n",
    "            \n",
    "            # Check if both concepts are in our mapping\n",
    "            if parent_ont_idx in ontology_to_concept and child_ont_idx in ontology_to_concept:\n",
    "                parent_concept_idx = ontology_to_concept[parent_ont_idx]\n",
    "                child_concept_idx = ontology_to_concept[child_ont_idx]\n",
    "                \n",
    "                # Check if they're in the same cluster (should be for good hierarchy preservation)\n",
    "                if labels[parent_concept_idx] == labels[child_concept_idx]:\n",
    "                    consistent_pairs += 1\n",
    "                \n",
    "                total_pairs += 1\n",
    "        \n",
    "        return consistent_pairs / total_pairs if total_pairs > 0 else 0\n",
    "    \n",
    "    def create_quality_report(self):\n",
    "        \"\"\"Create comprehensive quality assessment report.\"\"\"\n",
    "        print(f\"üìã Creating comprehensive quality report...\")\n",
    "        \n",
    "        # Gather all assessments\n",
    "        clustering_quality = self.assess_clustering_quality()\n",
    "        hierarchy_quality = self.assess_taxonomy_hierarchy()\n",
    "        validation_results = self.validate_against_original_ontology()\n",
    "        \n",
    "        # Create summary report\n",
    "        report = {\n",
    "            'dataset_info': {\n",
    "                'n_concepts': len(self.concept_df),\n",
    "                'embedding_dim': self.embeddings.shape[1],\n",
    "                'concept_types': dict(self.concept_df['concept_type'].value_counts())\n",
    "            },\n",
    "            'clustering_assessment': clustering_quality,\n",
    "            'hierarchy_assessment': hierarchy_quality,\n",
    "            'validation_results': validation_results\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Assess taxonomy quality\n",
    "if all(var in locals() for var in ['cluster_data', 'clustering_results', 'taxonomy_result']):\n",
    "    assessor = TaxonomyQualityAssessor(cluster_data, clustering_results, taxonomy_result)\n",
    "    \n",
    "    print(\"\\nüìè TAXONOMY QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create quality report\n",
    "    quality_report = assessor.create_quality_report()\n",
    "    \n",
    "    # Display clustering assessment\n",
    "    print(f\"\\nüéØ CLUSTERING QUALITY COMPARISON:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if quality_report['clustering_assessment']:\n",
    "        # Create comparison table\n",
    "        comparison_data = []\n",
    "        \n",
    "        for method, metrics in quality_report['clustering_assessment'].items():\n",
    "            comparison_data.append({\n",
    "                'Method': metrics['algorithm'],\n",
    "                'Clusters': metrics['n_clusters'],\n",
    "                'Silhouette': f\"{metrics['silhouette_score']:.3f}\",\n",
    "                'Balance': f\"{metrics['size_balance']:.3f}\",\n",
    "                'Coherence': f\"{metrics['semantic_coherence']:.3f}\",\n",
    "                'Davies-Bouldin': f\"{metrics['davies_bouldin_score']:.3f}\"\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Identify best method\n",
    "        best_method = max(quality_report['clustering_assessment'].items(),\n",
    "                         key=lambda x: x[1]['silhouette_score'])\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Clustering Method: {best_method[1]['algorithm']}\")\n",
    "        print(f\"   Silhouette Score: {best_method[1]['silhouette_score']:.3f}\")\n",
    "        print(f\"   Semantic Coherence: {best_method[1]['semantic_coherence']:.3f}\")\n",
    "        print(f\"   Number of Clusters: {best_method[1]['n_clusters']}\")\n",
    "    \n",
    "    # Display hierarchy assessment\n",
    "    if quality_report['hierarchy_assessment']:\n",
    "        print(f\"\\nüå≥ HIERARCHY QUALITY:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        hier_qual = quality_report['hierarchy_assessment']\n",
    "        print(f\"Number of Levels: {hier_qual['n_levels']}\")\n",
    "        print(f\"Overall Coherence: {hier_qual['overall_coherence']:.3f}\")\n",
    "        print(f\"Hierarchy Depth Score: {hier_qual['hierarchy_depth_score']:.3f}\")\n",
    "        \n",
    "        if 'best_level' in hier_qual:\n",
    "            best_level = hier_qual['best_level']\n",
    "            print(f\"\\nBest Taxonomy Level: {best_level['level']}\")\n",
    "            print(f\"  ‚Ä¢ Clusters: {best_level['n_clusters']}\")\n",
    "            print(f\"  ‚Ä¢ Silhouette: {best_level['silhouette_score']:.3f}\")\n",
    "            print(f\"  ‚Ä¢ Balance: {best_level['balance_score']:.3f}\")\n",
    "    \n",
    "    # Display validation results\n",
    "    if quality_report['validation_results'] and 'error' not in quality_report['validation_results']:\n",
    "        print(f\"\\n‚úÖ ONTOLOGY VALIDATION:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        for method, validation in quality_report['validation_results'].items():\n",
    "            print(f\"{method:15}: consistency = {validation['hierarchy_consistency']:.3f}, coverage = {validation['coverage']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Quality assessment completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Quality assessor not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Discovered Taxonomies\n",
    "\n",
    "Export the discovered taxonomies in various formats for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_taxonomy_results(cluster_data, clustering_results, taxonomy_result, quality_report, \n",
    "                           output_dir='taxonomy_results'):\n",
    "    \"\"\"Export comprehensive taxonomy results.\"\"\"\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    ontology_name = Path(cluster_data['ontology_file']).stem\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    exported_files = []\n",
    "    \n",
    "    print(f\"üìÅ Exporting taxonomy results to {output_dir}/...\")\n",
    "    \n",
    "    # 1. Clustering results CSV\n",
    "    if clustering_results:\n",
    "        # Get best clustering result\n",
    "        best_clustering = max(clustering_results.items(), \n",
    "                             key=lambda x: x[1].get('silhouette_score', -2))\n",
    "        best_method = best_clustering[0]\n",
    "        best_labels = best_clustering[1]['labels']\n",
    "        \n",
    "        # Create clustering results DataFrame\n",
    "        cluster_df = cluster_data['concept_df'].copy()\n",
    "        cluster_df['cluster_id'] = best_labels\n",
    "        cluster_df['clustering_method'] = best_method\n",
    "        \n",
    "        clustering_file = Path(output_dir) / f\"{ontology_name}_clustering_{timestamp}.csv\"\n",
    "        cluster_df.to_csv(clustering_file, index=False)\n",
    "        exported_files.append(str(clustering_file))\n",
    "        print(f\"  ‚úì Clustering results: {clustering_file}\")\n",
    "    \n",
    "    # 2. Hierarchical taxonomy JSON\n",
    "    if 'taxonomy_levels' in taxonomy_result:\n",
    "        taxonomy_file = Path(output_dir) / f\"{ontology_name}_taxonomy_{timestamp}.json\"\n",
    "        \n",
    "        # Prepare taxonomy data for JSON serialization\n",
    "        taxonomy_export = {\n",
    "            'metadata': {\n",
    "                'source_ontology': cluster_data['ontology_file'],\n",
    "                'generation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "                'method': taxonomy_result['method'],\n",
    "                'n_concepts': len(cluster_data['concept_df']),\n",
    "                'n_levels': len(taxonomy_result['taxonomy_levels'])\n",
    "            },\n",
    "            'taxonomy_levels': {}\n",
    "        }\n",
    "        \n",
    "        for level, level_data in taxonomy_result['taxonomy_levels'].items():\n",
    "            taxonomy_export['taxonomy_levels'][str(level)] = {\n",
    "                'n_clusters': level_data['n_clusters'],\n",
    "                'silhouette_score': level_data['silhouette_score'],\n",
    "                'clusters': {str(cid): {k: v for k, v in cinfo.items() if k != 'concept_list'} \n",
    "                           for cid, cinfo in level_data['clusters'].items()}\n",
    "            }\n",
    "        \n",
    "        import json\n",
    "        with open(taxonomy_file, 'w') as f:\n",
    "            json.dump(taxonomy_export, f, indent=2, default=str)\n",
    "        \n",
    "        exported_files.append(str(taxonomy_file))\n",
    "        print(f\"  ‚úì Hierarchical taxonomy: {taxonomy_file}\")\n",
    "    \n",
    "    # 3. Quality assessment report\n",
    "    if quality_report:\n",
    "        quality_file = Path(output_dir) / f\"{ontology_name}_quality_report_{timestamp}.json\"\n",
    "        \n",
    "        with open(quality_file, 'w') as f:\n",
    "            json.dump(quality_report, f, indent=2, default=str)\n",
    "        \n",
    "        exported_files.append(str(quality_file))\n",
    "        print(f\"  ‚úì Quality report: {quality_file}\")\n",
    "    \n",
    "    # 4. Human-readable taxonomy summary\n",
    "    summary_file = Path(output_dir) / f\"{ontology_name}_taxonomy_summary_{timestamp}.txt\"\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"CONCEPT TAXONOMY DISCOVERY REPORT\\n\")\n",
    "        f.write(f\"{'='*50}\\n\\n\")\n",
    "        f.write(f\"Source Ontology: {cluster_data['ontology_file']}\\n\")\n",
    "        f.write(f\"Generated: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Tool: on2vec clustering & taxonomy discovery\\n\\n\")\n",
    "        \n",
    "        # Dataset summary\n",
    "        f.write(f\"DATASET SUMMARY\\n\")\n",
    "        f.write(f\"{'-'*20}\\n\")\n",
    "        f.write(f\"Total concepts: {len(cluster_data['concept_df'])}\\n\")\n",
    "        f.write(f\"Embedding dimensions: {cluster_data['embeddings'].shape[1]}\\n\")\n",
    "        f.write(f\"Concept types: {dict(cluster_data['concept_df']['concept_type'].value_counts())}\\n\\n\")\n",
    "        \n",
    "        # Clustering results\n",
    "        if quality_report and 'clustering_assessment' in quality_report:\n",
    "            f.write(f\"CLUSTERING RESULTS\\n\")\n",
    "            f.write(f\"{'-'*20}\\n\")\n",
    "            \n",
    "            for method, metrics in quality_report['clustering_assessment'].items():\n",
    "                f.write(f\"{metrics['algorithm']:20}: {metrics['n_clusters']} clusters, \"\n",
    "                       f\"silhouette = {metrics['silhouette_score']:.3f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Best taxonomy level\n",
    "        if quality_report and 'hierarchy_assessment' in quality_report:\n",
    "            hier_qual = quality_report['hierarchy_assessment']\n",
    "            if 'best_level' in hier_qual:\n",
    "                f.write(f\"DISCOVERED TAXONOMY\\n\")\n",
    "                f.write(f\"{'-'*20}\\n\")\n",
    "                \n",
    "                best_level = hier_qual['best_level']\n",
    "                f.write(f\"Best taxonomic level: {best_level['level']}\\n\")\n",
    "                f.write(f\"Number of categories: {best_level['n_clusters']}\\n\")\n",
    "                f.write(f\"Quality score: {best_level['silhouette_score']:.3f}\\n\\n\")\n",
    "                \n",
    "                # Show categories from best level\n",
    "                if 'taxonomy_levels' in taxonomy_result:\n",
    "                    best_level_data = taxonomy_result['taxonomy_levels'][best_level['level']]\n",
    "                    \n",
    "                    f.write(f\"DISCOVERED CATEGORIES\\n\")\n",
    "                    f.write(f\"{'-'*25}\\n\")\n",
    "                    \n",
    "                    for cluster_id, cluster_info in sorted(best_level_data['clusters'].items(),\n",
    "                                                          key=lambda x: x[1]['size'], reverse=True):\n",
    "                        f.write(f\"Category {cluster_id}: {cluster_info['taxonomy_label']}\\n\")\n",
    "                        f.write(f\"  Size: {cluster_info['size']} concepts\\n\")\n",
    "                        f.write(f\"  Dominant type: {cluster_info['dominant_type']}\\n\")\n",
    "                        f.write(f\"  Representative: {cluster_info['representative_concept']}\\n\")\n",
    "                        f.write(f\"  Examples: {', '.join(cluster_info['concept_list'][:3])}\\n\\n\")\n",
    "        \n",
    "        # Usage instructions\n",
    "        f.write(f\"HOW TO USE THESE RESULTS\\n\")\n",
    "        f.write(f\"{'-'*30}\\n\")\n",
    "        f.write(f\"1. Review the clustering CSV file for concept assignments\\n\")\n",
    "        f.write(f\"2. Use the taxonomy JSON for programmatic access to hierarchy\\n\")\n",
    "        f.write(f\"3. Check quality metrics to assess reliability\\n\")\n",
    "        f.write(f\"4. Consider the discovered categories for domain organization\\n\\n\")\n",
    "    \n",
    "    exported_files.append(str(summary_file))\n",
    "    print(f\"  ‚úì Summary report: {summary_file}\")\n",
    "    \n",
    "    # 5. Network file for visualization tools (if hierarchy graph exists)\n",
    "    if 'hierarchy_graph' in locals():\n",
    "        network_file = Path(output_dir) / f\"{ontology_name}_taxonomy_network_{timestamp}.gexf\"\n",
    "        \n",
    "        try:\n",
    "            nx.write_gexf(hierarchy_graph, network_file)\n",
    "            exported_files.append(str(network_file))\n",
    "            print(f\"  ‚úì Network file (GEXF): {network_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not export network file: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Exported {len(exported_files)} files to {output_dir}/\")\n",
    "    \n",
    "    # Show file sizes\n",
    "    print(f\"\\nüìä Export Summary:\")\n",
    "    for file_path in exported_files:\n",
    "        file_size = Path(file_path).stat().st_size\n",
    "        print(f\"  ‚Ä¢ {Path(file_path).name}: {file_size:,} bytes\")\n",
    "    \n",
    "    return exported_files\n",
    "\n",
    "# Export taxonomy results\n",
    "if all(var in locals() for var in ['cluster_data', 'clustering_results', 'taxonomy_result', 'quality_report']):\n",
    "    exported_files = export_taxonomy_results(\n",
    "        cluster_data, clustering_results, taxonomy_result, quality_report\n",
    "    )\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ TAXONOMY DISCOVERY COMPLETE!\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"‚úÖ Multi-algorithm clustering analysis performed\")\n",
    "    print(f\"‚úÖ Hierarchical taxonomies constructed\")\n",
    "    print(f\"‚úÖ Quality assessment completed\")\n",
    "    print(f\"‚úÖ Interactive visualizations created\")\n",
    "    print(f\"‚úÖ Results exported in multiple formats\")\n",
    "    \n",
    "    # Best recommendations\n",
    "    if quality_report and 'clustering_assessment' in quality_report:\n",
    "        best_method = max(quality_report['clustering_assessment'].items(),\n",
    "                         key=lambda x: x[1]['silhouette_score'])\n",
    "        \n",
    "        print(f\"\\nüèÜ RECOMMENDATIONS:\")\n",
    "        print(f\"  ‚Ä¢ Best clustering method: {best_method[1]['algorithm']}\")\n",
    "        print(f\"  ‚Ä¢ Recommended clusters: {best_method[1]['n_clusters']}\")\n",
    "        print(f\"  ‚Ä¢ Quality score: {best_method[1]['silhouette_score']:.3f}\")\n",
    "        \n",
    "        if quality_report.get('hierarchy_assessment', {}).get('best_level'):\n",
    "            best_level = quality_report['hierarchy_assessment']['best_level']\n",
    "            print(f\"  ‚Ä¢ Best taxonomy level: {best_level['level']} ({best_level['n_clusters']} categories)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot export results - missing required data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated comprehensive clustering and taxonomy discovery capabilities using on2vec embeddings:\n",
    "\n",
    "### ‚úÖ Key Achievements:\n",
    "\n",
    "1. **Multi-Algorithm Clustering**: Applied K-Means, Hierarchical, DBSCAN, GMM, and Spectral clustering\n",
    "2. **Hierarchical Taxonomy Construction**: Built multi-level taxonomies with semantic labels\n",
    "3. **Quality Assessment**: Evaluated clustering quality using multiple metrics and validation approaches\n",
    "4. **Interactive Visualization**: Created dynamic visualizations for taxonomy exploration\n",
    "5. **Semantic Coherence Analysis**: Assessed how well clusters align with conceptual categories\n",
    "6. **Export Capabilities**: Generated results in multiple formats for further use\n",
    "\n",
    "### üéØ Practical Applications:\n",
    "\n",
    "- **Scientific Domain Organization**: Discover natural groupings in research areas\n",
    "- **Knowledge Base Restructuring**: Identify optimal organizational structures for ontologies\n",
    "- **Emerging Trend Detection**: Find new research themes and concept clusters\n",
    "- **Literature Classification**: Organize papers and resources by discovered topics\n",
    "- **Curriculum Development**: Structure educational content based on conceptual relationships\n",
    "\n",
    "### üìä Clustering Insights:\n",
    "\n",
    "The analysis revealed that:\n",
    "- **Different algorithms excel at different structures**: K-Means for balanced clusters, DBSCAN for density-based groups\n",
    "- **Hierarchical taxonomies provide flexibility**: Multiple organizational levels for different use cases\n",
    "- **Semantic coherence varies by domain**: Some domains naturally cluster better than others\n",
    "- **Quality metrics guide selection**: Silhouette score, balance, and coherence provide comprehensive assessment\n",
    "\n",
    "### üîß Technical Features:\n",
    "\n",
    "- **Scalable Clustering**: Efficient processing of large concept spaces\n",
    "- **Multi-Level Hierarchies**: Taxonomies at different granularities\n",
    "- **Quality Validation**: Comprehensive assessment against multiple criteria\n",
    "- **Interactive Exploration**: User-friendly visualization interfaces\n",
    "- **Export Integration**: Standard formats for downstream applications\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Dynamic Taxonomies**: Update classifications as new concepts are added\n",
    "2. **Cross-Domain Clustering**: Find relationships between different knowledge areas\n",
    "3. **Expert Validation**: Incorporate domain expert feedback to refine taxonomies\n",
    "4. **Temporal Analysis**: Track how conceptual clusters evolve over time\n",
    "5. **Application Integration**: Deploy discovered taxonomies in real knowledge systems\n",
    "\n",
    "The clustering and taxonomy discovery capabilities demonstrated here show how on2vec embeddings can reveal hidden organizational structures in knowledge domains, enabling more intuitive navigation and understanding of complex conceptual spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}